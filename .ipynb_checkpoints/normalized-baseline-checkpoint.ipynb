{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d26d0e-21f7-4de7-a2b6-77ab1a0e0a81",
   "metadata": {},
   "source": [
    "# Normalized Baseline\n",
    "\n",
    "After testing, the most ideal normalization was \"normalize_wav\" which normalizes the waveform to have a mean of zero (mean=0) and a standard deviation of unity (std=1) before computing the spectrogram. After several testing runs with different spectrogram generation parameters, the most ideal were determined to be: \n",
    "\n",
    "\r\n",
    "  \"spectrogram\": {\r\n",
    "    \"rate\": 3000,\r\n",
    "    \"window\": 0.032,\r\n",
    "    \"step\": 0.001,\r\n",
    "    \"freq_min\": 100,\r\n",
    "    \"freq_max\": 1200,\r\n",
    "    \"transforms\": [\r\n",
    "      {\r\n",
    "        \"name\": \"normalize\",\r\n",
    "        \"mean\": 0,\r\n",
    "        \"std\": 1.0\r\n",
    "      }\r\n",
    "    ],\r\n",
    "    \"window_func\": \"hamming\",\r\n",
    "    \"type\": \"MagSpectrogram\",\r\n",
    "    \"duration\": 1.0\r\n",
    "  }\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d82478-093f-4b37-aa28-b2755205e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kzammit\\Miniconda3\\envs\\ketos_env\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import os \n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix as confusion_matrix_sklearn\n",
    "\n",
    "from ketos.data_handling import selection_table as sl\n",
    "import ketos.data_handling.database_interface as dbi\n",
    "from ketos.data_handling.parsing import load_audio_representation\n",
    "from ketos.data_handling.data_feeding import BatchGenerator\n",
    "from ketos.neural_networks.resnet import ResNetInterface\n",
    "from ketos.audio.audio_loader import AudioFrameLoader, AudioLoader, SelectionTableIterator\n",
    "from ketos.audio.spectrogram import MagSpectrogram\n",
    "from ketos.neural_networks.dev_utils.detection import batch_load_audio_file_data, filter_by_threshold, filter_by_label, merge_overlapping_detections\n",
    "from ketos.data_handling.data_feeding import JointBatchGen\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print('done importing packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f10d5-3d94-4e2d-b90e-a65ca95a90f8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115cbe28-82ef-4f75-8b2c-78bbcc1cb828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(file_durations, table):\n",
    "\n",
    "    drop = []\n",
    "\n",
    "    print('cleaning training table of original length ' + str(len(table)))\n",
    "\n",
    "    for idex, row in table.iterrows():\n",
    "    \n",
    "        # filename is row[0], end time is idex.end\n",
    "        index = file_durations.loc[file_durations['filename'] == row.name[0]].index\n",
    "        duration = file_durations['duration'][index].values[0]\n",
    "    \n",
    "        if duration < row.end:\n",
    "            # drop the row corresponding to that sel_id and filename from the dataframe\n",
    "            drop.append(idex)\n",
    "    \n",
    "        if row.start < 0:\n",
    "            drop.append(idex)\n",
    "    \n",
    "    print('Number of rows to drop (note, one entry may be in list twice): ' + str(len(drop)))\n",
    "\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c48753-b685-4198-854d-639a7a4e0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(log_folder):\n",
    "\n",
    "    files = glob.glob(log_folder  + \"/*.csv\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 5), sharey=True)\n",
    "    #axes.set_ylim(0.1, 0.20)\n",
    "    \n",
    "    for file in files:\n",
    "    \n",
    "        # Read the log file \n",
    "        log_file = pd.read_csv(file)\n",
    "    \n",
    "        labell = file.split('\\\\')[-1].split(\".\")[0]\n",
    "        \n",
    "        # Get the training and validation losses \n",
    "        tr_results = log_file[log_file['dataset']=='train']\n",
    "        va_results = log_file[log_file['dataset']=='val']\n",
    "        \n",
    "        # Plot the loss curves \n",
    "        sns.lineplot(data=tr_results, x='epoch', y='loss', label= str(labell) + ' train', legend='auto')\n",
    "        sns.lineplot(data=va_results, x='epoch', y='loss', label=str(labell) + ' val', legend='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d18cd0f-53c7-46d9-b957-03b237a2f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detections(labels, scores, threshold=0.5):\n",
    "\n",
    "    # Compute the positive scores above threshold, 1 if it is above threshold, 0 if it is not \n",
    "    predictions = np.where(scores >= threshold, 1,0)\n",
    "\n",
    "    # TP: Does the annotated label match the prediction above threshold? Bc \"scores\" is defined as the positive threshold, this represents TP\n",
    "    TP = tf.math.count_nonzero(predictions * labels).numpy()\n",
    "\n",
    "    # TN: Negative score is \"predictions - 1\" bc predictions was for the positive result, labels-1 so that the negatives are multiplied by 1\n",
    "    TN = tf.math.count_nonzero((predictions - 1) * (labels - 1)).numpy()\n",
    "\n",
    "    # And so on \n",
    "    FP = tf.math.count_nonzero(predictions * (labels - 1)).numpy()\n",
    "    FN = tf.math.count_nonzero((predictions - 1) * labels).numpy()\n",
    "\n",
    "    return predictions, TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab67e38-3f77-4edb-82a2-623904869965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_plot(cf, output_folder,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=True):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "\n",
    "    title:         Title for the heatmap. Default is None.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names) == cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten() / np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels, group_counts, group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0], cf.shape[1])\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        # Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        # if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf) == 2:\n",
    "            # Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1, 1] / sum(cf[:, 1])\n",
    "            recall = cf[1, 1] / sum(cf[1, :])\n",
    "            f1_score = 2 * precision * recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy, precision, recall, f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize == None:\n",
    "        # Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks == False:\n",
    "        # Do not show categories if xyticks is False\n",
    "        categories = False\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf, annot=box_labels, fmt=\"\", cmap=cmap, cbar=cbar, xticklabels=categories, yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        #plt.xlabel('Predicted label' + stats_text)\n",
    "        plt.xlabel('Predicted label')\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "\n",
    "    if title:\n",
    "        #plt.title(title)\n",
    "        #plt.title(stats_text)\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    plt.savefig(output_folder + '\\\\' + 'confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c649d-fec7-4891-abdc-3f21272b6787",
   "metadata": {},
   "source": [
    "## Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c987125c-1923-4050-a61c-5866eae63543",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = r'E:\\baseline-with-normalization'\n",
    "spectro_file = main_folder + '\\\\' + 'spec_config_100-1200Hz-0.032-hamm-normalized.json'\n",
    "recipe_file = main_folder + '\\\\' + 'resnet_recipe-1.json'\n",
    "pos_folder = main_folder + '\\\\' + r'annots\\pos'\n",
    "neg_folder = main_folder + '\\\\' + r'annots\\neg'\n",
    "file_durations_file = main_folder + '\\\\' + 'all_file_durations_complete.xlsx'\n",
    "file_durations = pd.read_excel(file_durations_file)\n",
    "\n",
    "db_name = main_folder + '\\\\' + 'final-baseline-db-normalized.h5'\n",
    "model_name = main_folder + '\\\\' + 'final-baseline-model-normalized.kt'\n",
    "temp_folder = main_folder + '\\\\' + 'rs-temp'\n",
    "\n",
    "data_folder = r'D:\\ringed-seal-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a04199d-fb6b-4ca0-b4a3-0101c3e4dbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ulu22 vals (tr, va, te): [1037, 296, 143], total: 1476\n",
      "ulu vals (tr, va, te): [669, 191, 95], total: 955\n",
      "kk vals (tr, va, te): [1348, 384, 192], total: 1924\n",
      "cb vals (tr, va, te): [133, 38, 19], total: 190\n"
     ]
    }
   ],
   "source": [
    "# Get list of all csv files in positives annot folder \n",
    "files_pos = glob.glob(pos_folder + \"/*.csv\")\n",
    "\n",
    "site_names = []\n",
    "num_annots = []\n",
    "\n",
    "# For each csv file\n",
    "for file in files_pos:\n",
    "\n",
    "    annots = pd.read_csv(file)\n",
    "\n",
    "    site_name = file.split('\\\\')[-1].split('.')[0].split('_')[0]\n",
    "\n",
    "    site_names.append(site_name)\n",
    "    num_annots.append(len(annots))\n",
    "\n",
    "ULU22_val = num_annots[site_names.index('ULU2022')]\n",
    "other_val = sum(num_annots) - ULU22_val\n",
    "all_annots = sum(num_annots)\n",
    "\n",
    "ulu_2022_split = 0.32\n",
    "all_else_split = 1 - ulu_2022_split\n",
    "\n",
    "dataset_split = [0.7, 0.2, 0.1]\n",
    "\n",
    "train_annots = round(all_annots*dataset_split[0])\n",
    "val_annots = round(all_annots*dataset_split[1])\n",
    "test_annots = round(all_annots*dataset_split[2])\n",
    "\n",
    "# ulu22 vals\n",
    "ulu22_tr = round(train_annots*ulu_2022_split)\n",
    "ulu22_va = round(val_annots*ulu_2022_split)\n",
    "ulu22_te = round(test_annots*ulu_2022_split)\n",
    "\n",
    "ulu22_leftovers = ULU22_val - ulu22_tr - ulu22_va - ulu22_te\n",
    "\n",
    "if ulu22_leftovers < 0:\n",
    "    ulu22_te = ulu22_te + ulu22_leftovers\n",
    "\n",
    "if ulu22_leftovers > 0 :\n",
    "    ulu22_tr = ulu22_tr + ulu22_leftovers\n",
    "\n",
    "all_ulu = ulu22_tr + ulu22_te + ulu22_va\n",
    "\n",
    "if all_ulu != ULU22_val:\n",
    "    print('Something went wrong with Ulu')\n",
    "    exit()\n",
    "\n",
    "ulu2022_vals = [ulu22_tr, ulu22_va, ulu22_te]\n",
    "\n",
    "# rest vals\n",
    "rest_tr = round(train_annots*all_else_split)\n",
    "rest_va = round(val_annots*all_else_split)\n",
    "rest_te = round(test_annots*all_else_split)\n",
    "\n",
    "# totals\n",
    "all_added = rest_tr + rest_va + rest_te\n",
    "\n",
    "if all_added < other_val:\n",
    "    leftover = other_val - all_added\n",
    "    rest_tr = rest_tr + leftover\n",
    "\n",
    "if all_added > other_val:\n",
    "    leftover = all_added - other_val\n",
    "    rest_va = rest_va - leftover\n",
    "\n",
    "all_added2 = rest_tr + rest_va + rest_te\n",
    "\n",
    "cb_perc = num_annots[0]/all_added2\n",
    "kk_perc = num_annots[1]/all_added2\n",
    "ulu_perc = num_annots[4]/all_added2\n",
    "\n",
    "# split into other site vals\n",
    "cb_tr = round(cb_perc*rest_tr)\n",
    "cb_va = round(cb_perc*rest_va)\n",
    "cb_te = round(cb_perc*rest_te)\n",
    "total_cb = cb_tr + cb_va + cb_te\n",
    "if total_cb < num_annots[0]:\n",
    "    leftover_cb = num_annots[0] - total_cb\n",
    "    cb_tr = cb_tr + leftover_cb\n",
    "if total_cb > num_annots[0]:\n",
    "    leftover_cb = num_annots[0] - total_cb\n",
    "    cb_va = cb_va + leftover_cb\n",
    "cb_vals = [cb_tr, cb_va, cb_te]\n",
    "\n",
    "kk_tr = round(kk_perc*rest_tr)\n",
    "kk_va = round(kk_perc*rest_va)\n",
    "kk_te = round(kk_perc*rest_te)\n",
    "total_kk = kk_tr + kk_va + kk_te\n",
    "if total_kk < num_annots[1]:\n",
    "    leftover_kk = num_annots[1] - total_kk\n",
    "    kk_tr = kk_tr + leftover_kk\n",
    "if total_kk > num_annots[1]:\n",
    "    leftover_kk = num_annots[1] - total_kk\n",
    "    kk_va = kk_va + leftover_kk\n",
    "kk_vals = [kk_tr, kk_va, kk_te]\n",
    "    \n",
    "ulu_tr = round(ulu_perc*rest_tr)\n",
    "ulu_va = round(ulu_perc*rest_va)\n",
    "ulu_te = round(ulu_perc*rest_te)\n",
    "total_ulu = ulu_tr + ulu_va + ulu_te\n",
    "if total_ulu < num_annots[4]:\n",
    "    leftover_ulu = num_annots[4] - total_ulu\n",
    "    ulu_tr = ulu_tr + leftover_ulu\n",
    "if total_ulu > num_annots[4]:\n",
    "    leftover_ulu = num_annots[4] - total_ulu\n",
    "    ulu_va = ulu_va + leftover_ulu\n",
    "ulu_vals = [ulu_tr, ulu_va, ulu_te]\n",
    "\n",
    "print('ulu22 vals (tr, va, te): ' + str(ulu2022_vals) + ', total: ' + str(sum(ulu2022_vals)))\n",
    "print('ulu vals (tr, va, te): ' + str(ulu_vals) + ', total: ' + str(sum(ulu_vals)))\n",
    "print('kk vals (tr, va, te): ' + str(kk_vals) + ', total: ' + str(sum(kk_vals)))\n",
    "print('cb vals (tr, va, te): ' + str(cb_vals) + ', total: ' + str(sum(cb_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37bde5c3-c394-4b48-8961-6a76819c0f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives standardized? True\n",
      "Negatives standardized? True\n",
      "Negatives standardized? True\n",
      "Negatives standardized? True\n",
      "Positives standardized? True\n",
      "Positives standardized? True\n",
      "Positives standardized? True\n",
      "Positives standardized? True\n"
     ]
    }
   ],
   "source": [
    "## Create Database ##\n",
    "\n",
    "# negatives tables and standarize for ketos\n",
    "ulu_neg = pd.read_excel(neg_folder + '\\\\' + 'ULU-negs-joined.xlsx')\n",
    "ulu_neg = ulu_neg.ffill()\n",
    "ulu_neg = sl.standardize(table=ulu_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(ulu_neg)))\n",
    "\n",
    "ulu2022_neg = pd.read_excel(neg_folder + '\\\\' + 'ULU2022-negs-joined.xlsx')\n",
    "ulu2022_neg = ulu2022_neg.ffill()\n",
    "ulu2022_neg = sl.standardize(table=ulu2022_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(ulu2022_neg)))\n",
    "\n",
    "kk_neg = pd.read_excel(neg_folder + '\\\\' + 'KK-negs-joined.xlsx')\n",
    "kk_neg = kk_neg.ffill()\n",
    "kk_neg = sl.standardize(table=kk_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(kk_neg)))\n",
    "\n",
    "cb_neg = pd.read_excel(neg_folder + '\\\\' + 'CB-negs-joined.xlsx')\n",
    "cb_neg = cb_neg.ffill()\n",
    "cb_neg = sl.standardize(table=cb_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(cb_neg)))\n",
    "\n",
    "# positives tables\n",
    "ulu_pos = pd.read_csv(pos_folder + '\\\\' + 'ULU_all_formatted_1sec.csv')\n",
    "ulu_pos = ulu_pos.ffill()\n",
    "ulu_pos = sl.standardize(table=ulu_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(ulu_pos)))\n",
    "\n",
    "ulu2022_pos = pd.read_csv(pos_folder + '\\\\' + 'ULU2022_all_formatted_1sec.csv')\n",
    "ulu2022_pos = ulu2022_pos.ffill()\n",
    "ulu2022_pos = sl.standardize(table=ulu2022_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(ulu2022_pos)))\n",
    "\n",
    "kk_pos = pd.read_csv(pos_folder + '\\\\' + 'KK_all_formatted_1sec.csv')\n",
    "kk_pos = kk_pos.ffill()\n",
    "kk_pos = sl.standardize(table=kk_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(kk_pos)))\n",
    "\n",
    "cb_pos = pd.read_csv(pos_folder + '\\\\' + 'CB_all_formatted_1sec.csv')\n",
    "cb_pos = cb_pos.ffill()\n",
    "cb_pos = sl.standardize(table=cb_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(cb_pos)))\n",
    "\n",
    "# join into complete tables\n",
    "\n",
    "ulu_pos_tr = ulu_pos.head(ulu_vals[0])\n",
    "ulu_pos_va = ulu_pos[ulu_vals[0]:ulu_vals[0] + ulu_vals[1]]\n",
    "ulu_pos_te = ulu_pos.tail(ulu_vals[2])\n",
    "\n",
    "ulu_neg_tr = ulu_neg.head(ulu_vals[0])\n",
    "ulu_neg_va = ulu_neg[ulu_vals[0]:ulu_vals[0] + ulu_vals[1]]\n",
    "ulu_neg_te = ulu_neg.tail(ulu_vals[2])\n",
    "\n",
    "ulu_tr = pd.concat([ulu_pos_tr, ulu_neg_tr])\n",
    "ulu_va = pd.concat([ulu_pos_va, ulu_neg_va])\n",
    "ulu_te = pd.concat([ulu_pos_te, ulu_neg_te])\n",
    "\n",
    "ulu2022_pos_tr = ulu2022_pos.head(ulu2022_vals[0])\n",
    "ulu2022_pos_va = ulu2022_pos[ulu2022_vals[0]:ulu2022_vals[0] + ulu2022_vals[1]]\n",
    "ulu2022_pos_te = ulu2022_pos.tail(ulu2022_vals[2])\n",
    "\n",
    "ulu2022_neg_tr = ulu2022_neg.head(ulu2022_vals[0])\n",
    "ulu2022_neg_va = ulu2022_neg[ulu2022_vals[0]:ulu2022_vals[0] + ulu2022_vals[1]]\n",
    "ulu2022_neg_te = ulu2022_neg.tail(ulu2022_vals[2])\n",
    "\n",
    "ulu2022_tr = pd.concat([ulu2022_pos_tr, ulu2022_neg_tr])\n",
    "ulu2022_va = pd.concat([ulu2022_pos_va, ulu2022_neg_va])\n",
    "ulu2022_te = pd.concat([ulu2022_pos_te, ulu2022_neg_te])\n",
    "\n",
    "kk_pos_tr = kk_pos.head(kk_vals[0])\n",
    "kk_pos_va = kk_pos[kk_vals[0]:kk_vals[0] + kk_vals[1]]\n",
    "kk_pos_te = kk_pos.tail(kk_vals[2])\n",
    "\n",
    "kk_neg_tr = kk_neg.head(kk_vals[0])\n",
    "kk_neg_va = kk_neg[kk_vals[0]:kk_vals[0] + kk_vals[1]]\n",
    "kk_neg_te = kk_neg.tail(kk_vals[2])\n",
    "\n",
    "kk_tr = pd.concat([kk_pos_tr, kk_neg_tr])\n",
    "kk_va = pd.concat([kk_pos_va, kk_neg_va])\n",
    "kk_te = pd.concat([kk_pos_te, kk_neg_te])\n",
    "\n",
    "cb_pos_tr = cb_pos.head(cb_vals[0])\n",
    "cb_pos_va = cb_pos[cb_vals[0]:cb_vals[0] + cb_vals[1]]\n",
    "cb_pos_te = cb_pos.tail(cb_vals[2])\n",
    "\n",
    "cb_neg_tr = cb_neg.head(cb_vals[0])\n",
    "cb_neg_va = cb_neg[cb_vals[0]:cb_vals[0] + cb_vals[1]]\n",
    "cb_neg_te = cb_neg.tail(cb_vals[2])\n",
    "\n",
    "cb_tr = pd.concat([cb_pos_tr, cb_neg_tr])\n",
    "cb_va = pd.concat([cb_pos_va, cb_neg_va])\n",
    "cb_te = pd.concat([cb_pos_te, cb_neg_te])\n",
    "\n",
    "# final three tables\n",
    "\n",
    "train = pd.concat([ulu_tr, ulu2022_tr, cb_tr, kk_tr])\n",
    "val = pd.concat([ulu_va, ulu2022_va, cb_va, kk_va])\n",
    "test = pd.concat([ulu_te, ulu2022_te, cb_te, kk_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da6c4b78-440b-46ca-8a5f-62485ce6e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning training table of original length 6374\n",
      "Number of rows to drop (note, one entry may be in list twice): 10\n",
      "6362\n",
      "cleaning training table of original length 1818\n",
      "Number of rows to drop (note, one entry may be in list twice): 4\n",
      "1813\n",
      "cleaning training table of original length 898\n",
      "Number of rows to drop (note, one entry may be in list twice): 2\n",
      "894\n"
     ]
    }
   ],
   "source": [
    "drop_rows_tr = drop_rows(file_durations, train)\n",
    "train = train.drop(drop_rows_tr)\n",
    "print(len(train))\n",
    "drop_rows_va = drop_rows(file_durations, val)\n",
    "val = val.drop(drop_rows_va)\n",
    "print(len(val))\n",
    "drop_rows_te = drop_rows(file_durations, test)\n",
    "test = test.drop(drop_rows_te)\n",
    "print(len(test))\n",
    "\n",
    "train.to_csv(main_folder + '\\\\' + 'train.csv')\n",
    "val.to_csv(main_folder + '\\\\' + 'val.csv')\n",
    "test.to_csv(main_folder + '\\\\' + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b505ca-bc85-46d6-85fa-9860e5464576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▏                                                       | 1808/6362 [00:53<02:07, 35.60it/s]"
     ]
    }
   ],
   "source": [
    "# join into a database\n",
    "# Load the spectrogram representation & parameters, this returns a dict \n",
    "spec_cfg = load_audio_representation(spectro_file, name=\"spectrogram\")\n",
    "\n",
    "# Create a table called \"train\" in the database, defined by db_name, using the \"train\" selections table, the spectrogram config, and the audio data \n",
    "# Behind the hood, this creates an AudioLoader and AudioWriter Ketos function which generates the spectrograms for each selection \n",
    "dbi.create_database(output_file=db_name,  # empty brackets\n",
    "                    dataset_name=r'train', selections=train, data_dir=data_folder,\n",
    "                    audio_repres=spec_cfg)\n",
    "\n",
    "dbi.create_database(output_file=db_name,  # empty brackets\n",
    "                    dataset_name=r'val', selections=val, data_dir=data_folder,\n",
    "                    audio_repres=spec_cfg)\n",
    "\n",
    "dbi.create_database(output_file=db_name,  # empty brackets\n",
    "                    dataset_name=r'test', selections=test, data_dir=data_folder,\n",
    "                    audio_repres=spec_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e287a0-d281-4918-a3e2-42af4cc6ecf3",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5238dc2-c741-4b46-80f1-8dec6321e2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ketos_env",
   "language": "python",
   "name": "ketos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
