{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0a33e7-ddae-42b8-a1ae-59340f1bce1e",
   "metadata": {},
   "source": [
    "# Fine-Tuning Pearce Point w Cierra's Annots 2024-06-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933477bc-8c6a-4ab8-82c5-d34da0f6703b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T21:49:17.982152Z",
     "iopub.status.busy": "2024-06-13T21:49:17.982152Z",
     "iopub.status.idle": "2024-06-13T21:49:29.811530Z",
     "shell.execute_reply": "2024-06-13T21:49:29.811530Z",
     "shell.execute_reply.started": "2024-06-13T21:49:17.982152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kzammit\\Miniconda3\\envs\\ketos_env\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix as confusion_matrix_sklearn\n",
    "import scipy\n",
    "\n",
    "from ketos.data_handling import selection_table as sl\n",
    "import ketos.data_handling.database_interface as dbi\n",
    "from ketos.data_handling.parsing import load_audio_representation\n",
    "from ketos.data_handling.data_feeding import BatchGenerator\n",
    "from ketos.neural_networks.resnet import ResNetInterface\n",
    "from ketos.audio.audio_loader import AudioFrameLoader, AudioLoader, SelectionTableIterator\n",
    "from ketos.audio.spectrogram import MagSpectrogram\n",
    "from ketos.neural_networks.dev_utils.detection import batch_load_audio_file_data, filter_by_threshold, filter_by_label, merge_overlapping_detections\n",
    "from ketos.data_handling.data_feeding import JointBatchGen\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print('done importing packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79472cdf-ca0a-4dbb-90f9-fb2a060917ba",
   "metadata": {},
   "source": [
    "Manually created the \"NORS-FORMATTED\" files, added end with the file durations file I have in the lockbox folder\n",
    "Repeated the below code for each site (CB50, CB300, PP)\n",
    "For the first try for fine-tuning I'll use just PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40453282-79af-4d82-aa6f-7c38da9f02de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T20:49:39.190427Z",
     "iopub.status.busy": "2024-06-13T20:49:39.190427Z",
     "iopub.status.idle": "2024-06-13T20:49:39.202007Z",
     "shell.execute_reply": "2024-06-13T20:49:39.200493Z",
     "shell.execute_reply.started": "2024-06-13T20:49:39.190427Z"
    }
   },
   "outputs": [],
   "source": [
    "#annot_neg_pp = pd.read_excel(r'E:\\baseline-with-normalization-reduce-tonal\\fine-tuning\\CierrasFiles\\formatted\\CB50-ICE-NORS-FORMATTED.xlsx')\n",
    "#std_annot_neg_pp = sl.standardize(table=annot_neg_pp, trim_table=True)\n",
    "#pp_neg_add = sl.select(annotations=std_annot_neg_pp, length=1.0, step=1, min_overlap=1, center=False)\n",
    "#pp_neg_add['Class'] = 'C'\n",
    "#pp_neg_add.to_excel(r'E:\\baseline-with-normalization-reduce-tonal\\fine-tuning\\CierrasFiles\\formatted\\new_negatives_cb.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2bb306-9062-4d7e-b419-725486eb6980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T21:49:29.811530Z",
     "iopub.status.busy": "2024-06-13T21:49:29.811530Z",
     "iopub.status.idle": "2024-06-13T21:50:16.815253Z",
     "shell.execute_reply": "2024-06-13T21:50:16.815253Z",
     "shell.execute_reply.started": "2024-06-13T21:49:29.811530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives standardized? TrueTrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1340/1340 [00:36<00:00, 36.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340 items saved to E:\\baseline-with-normalization-reduce-tonal\\fine-tuning\\pp-w-neg-cierra.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 238/238 [00:04<00:00, 53.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238 items saved to E:\\baseline-with-normalization-reduce-tonal\\fine-tuning\\pp-w-neg-cierra.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# because of the weird ketos merging, read in the excel sheet instead of using the above\n",
    "annot_neg_pp = pd.read_excel(r'E:\\baseline-with-normalization-reduce-tonal\\fine-tuning\\annots\\neg\\dev\\new_negatives_pp.xlsx')\n",
    "db_name = r'E:\\baseline-with-normalization-reduce-tonal\\fine-tuning\\pp-w-neg-cierra.h5'\n",
    "data_folder = r'D:\\ringed-seal-data'\n",
    "file_durations = pd.read_excel(r'E:\\baseline-with-normalization-reduce-tonal\\all_file_durations_complete.xlsx')\n",
    "\n",
    "annot_neg_pp = annot_neg_pp.ffill()\n",
    "\n",
    "pp_tr_neg = annot_neg_pp.head(int(len(annot_neg_pp)*(85/100)))\n",
    "pp_te_neg = annot_neg_pp[~annot_neg_pp.index.isin(pp_tr_neg.index)]\n",
    "\n",
    "pp_tr_neg = sl.standardize(table=pp_tr_neg)\n",
    "pp_te_neg = sl.standardize(table=pp_te_neg)\n",
    "\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(pp_tr_neg)) + str(sl.is_standardized(pp_te_neg)))\n",
    "\n",
    "pp_pos = pd.read_csv(r'E:\\baseline-with-normalization-reduce-tonal\\fine-tuning\\annots\\pos\\PP_all_formatted_1sec.csv')\n",
    "pp_pos = pp_pos.ffill()\n",
    "pp_tr_pos = pp_pos.head(int(len(pp_pos)*(85/100)))\n",
    "pp_te_pos = pp_pos[~pp_pos.index.isin(pp_tr_pos.index)]\n",
    "\n",
    "pp_tr_pos = sl.standardize(table=pp_tr_pos, start_labels_at_1=True)\n",
    "pp_te_pos = sl.standardize(table=pp_te_pos, start_labels_at_1=True)\n",
    "\n",
    "pp_tr = pd.concat([pp_tr_pos, pp_tr_neg])\n",
    "pp_te = pd.concat([pp_te_pos, pp_te_neg])\n",
    "\n",
    "pp_tr.to_excel(r'E:\\baseline-with-normalization-reduce-tonal\\fine-tuning\\train.xlsx')\n",
    "pp_te.to_excel(r'E:\\baseline-with-normalization-reduce-tonal\\fine-tuning\\test.xlsx')\n",
    "\n",
    "spectro_file = r'E:\\baseline-with-normalization-reduce-tonal\\spec_config_100-1200Hz-0.032-hamm-normalized-reduce-tonal.json'\n",
    "spec_cfg = load_audio_representation(spectro_file, name=\"spectrogram\")\n",
    "\n",
    "dbi.create_database(output_file=db_name,  # empty brackets\n",
    "                    dataset_name=r'train', selections=pp_tr, data_dir=data_folder,\n",
    "                    audio_repres=spec_cfg)\n",
    "\n",
    "dbi.create_database(output_file=db_name,  # empty brackets\n",
    "                    dataset_name=r'test', selections=pp_te, data_dir=data_folder,\n",
    "                    audio_repres=spec_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8aa2b7-9883-4a93-8929-efbc6d9d500b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T21:50:29.199453Z",
     "iopub.status.busy": "2024-06-13T21:50:29.199453Z",
     "iopub.status.idle": "2024-06-13T21:50:29.220974Z",
     "shell.execute_reply": "2024-06-13T21:50:29.220974Z",
     "shell.execute_reply.started": "2024-06-13T21:50:29.199453Z"
    }
   },
   "outputs": [],
   "source": [
    "main_folder = r'E:\\baseline-with-normalization-reduce-tonal\\fine-tuning'\n",
    "\n",
    "model_folder = r'E:\\baseline-with-normalization-reduce-tonal\\models'\n",
    "\n",
    "pretrained_models = [model_folder + '\\\\' + 'rs-model-0.kt', model_folder + '\\\\' + 'rs-model-1.kt', model_folder + '\\\\' + 'rs-model-2.kt', \n",
    "                     model_folder + '\\\\' + 'rs-model-3.kt', model_folder + '\\\\' + 'rs-model-4.kt', model_folder + '\\\\' + 'rs-model-5.kt',\n",
    "                    model_folder + '\\\\' + 'rs-model-6.kt', model_folder + '\\\\' + 'rs-model-7.kt', model_folder + '\\\\' + 'rs-model-8.kt',\n",
    "                    model_folder + '\\\\' + 'rs-model-9.kt']\n",
    "\n",
    "new_models = [main_folder + '\\\\' + 'rs-model-0-ft.kt', main_folder + '\\\\' + 'rs-model-1-ft.kt', main_folder + '\\\\' + 'rs-model-2-ft.kt', \n",
    "              main_folder + '\\\\' + 'rs-model-3-ft.kt', main_folder + '\\\\' + 'rs-model-4-ft.kt', main_folder + '\\\\' + 'rs-model-5-ft.kt',\n",
    "             main_folder + '\\\\' + 'rs-model-6-ft.kt', main_folder + '\\\\' + 'rs-model-7-ft.kt', main_folder + '\\\\' + 'rs-model-8-ft.kt',\n",
    "              main_folder + '\\\\' + 'rs-model-9-ft.kt']\n",
    "\n",
    "np_seeds = [1736, 680, 1996, 1522, 867, 543, 249, 707, 584, 1236, 161]\n",
    "tf_seeds = [1660, 977, 1396, 1456, 1539, 673, 1743, 1492, 1776, 1273, 394]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf1537b-e98d-47d4-9fff-e518ef5b404e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T21:51:16.299029Z",
     "iopub.status.busy": "2024-06-13T21:51:16.299029Z",
     "iopub.status.idle": "2024-06-13T21:55:08.635886Z",
     "shell.execute_reply": "2024-06-13T21:55:08.635125Z",
     "shell.execute_reply.started": "2024-06-13T21:51:16.299029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the batch size and number of epochs for training\n",
    "batch_size = 16\n",
    "n_epochs = 10\n",
    "\n",
    "for idx, model in enumerate(pretrained_models):\n",
    "    \n",
    "    # Set the random seed for numpy and tensorflow\n",
    "    np.random.seed(np_seeds[idx])\n",
    "    tf.random.set_seed(tf_seeds[idx])\n",
    "\n",
    "    # Set the log folder and checkpoint folder\n",
    "    log_folder = main_folder + '\\\\' + 'logs' + str(idx)\n",
    "    checkpoint_folder = main_folder + '\\\\' + 'checkpoints' +str(idx)\n",
    "    \n",
    "    # Open the database file in read mode\n",
    "    db = dbi.open_file(db_name, 'r')\n",
    "    \n",
    "    # Open the training and validation tables respectively\n",
    "    train_data = dbi.open_table(db, \"/train/data\")\n",
    "    val_data = dbi.open_table(db, \"/test/data\")\n",
    "    \n",
    "    # Create batches of training data of size batch size, using the specified data table\n",
    "    # This returns indices of the data in each batch along with their labels\n",
    "    train_generator = BatchGenerator(batch_size=batch_size, data_table=train_data,\n",
    "                                        output_transform_func=ResNetInterface.transform_batch,\n",
    "                                        shuffle=True, refresh_on_epoch_end=True)\n",
    "    \n",
    "    # Create batches of validation data of size batch size, using the specified data table \n",
    "    # This returns indices of the data in each batch along with their labels \n",
    "    val_generator = BatchGenerator(batch_size=batch_size, data_table=val_data,\n",
    "                                       output_transform_func=ResNetInterface.transform_batch,\n",
    "                                       shuffle=False, refresh_on_epoch_end=False)\n",
    "    \n",
    "    # Load the pretrained model, replacing the top (aka. classification layers). This method inherently freezes the base.\n",
    "    resnet = ResNetInterface.load(model, replace_top=True)\n",
    "    \n",
    "    # Set the training and validation generators to the batch generators created above\n",
    "    resnet.train_generator = train_generator\n",
    "    resnet.val_generator = val_generator\n",
    "    \n",
    "    # Set the model log and checkpoint directory\n",
    "    resnet.log_dir = log_folder\n",
    "    resnet.checkpoint_dir = checkpoint_folder\n",
    "    \n",
    "    # digging into their scripts for fine tuning\n",
    "    # resnet.py, \"clone with new top\"\n",
    "    \n",
    "    # Train the model, looping through all of the training and validation data\n",
    "    # See code map for more information\n",
    "    resnet.train_loop(n_epochs=n_epochs, verbose=False, log_csv=True, csv_name='log-' + str(idx) +'.csv', validate=True)\n",
    "    #resnet.train_loop(n_epochs=n_epochs, verbose=False, log_csv=True, csv_name='log.csv', validate=False)\n",
    "    \n",
    "    # Close the database\n",
    "    db.close()\n",
    "    \n",
    "    # Save the model file, and keep track of the spectrogram parameters used to generate that model\n",
    "    resnet.save(new_models[idx], audio_repr_file=spectro_file)\n",
    "    \n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14cfed-c0de-4a60-8c68-bbf58c73ddac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ketos_env",
   "language": "python",
   "name": "ketos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
