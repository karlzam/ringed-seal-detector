{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1015b30f-4953-4af3-94f6-f87f07fe3477",
   "metadata": {},
   "source": [
    "# Tests - 20240116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da62faf2-acd0-42b5-b9ed-e6abf5b7d2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing packages\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import os \n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix as confusion_matrix_sklearn\n",
    "\n",
    "from ketos.data_handling import selection_table as sl\n",
    "import ketos.data_handling.database_interface as dbi\n",
    "from ketos.data_handling.parsing import load_audio_representation\n",
    "from ketos.data_handling.data_feeding import BatchGenerator\n",
    "from ketos.neural_networks.resnet import ResNetInterface\n",
    "from ketos.audio.audio_loader import AudioFrameLoader, AudioLoader, SelectionTableIterator\n",
    "from ketos.audio.spectrogram import MagSpectrogram\n",
    "from ketos.neural_networks.dev_utils.detection import batch_load_audio_file_data, filter_by_threshold\n",
    "from ketos.data_handling.data_feeding import JointBatchGen\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print('done importing packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70629cb-702f-4656-ad36-035ee4a30d44",
   "metadata": {},
   "source": [
    "## Manual Dataset Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f531c2-b5d8-4e64-bba4-a4bd0e9a7e15",
   "metadata": {},
   "source": [
    "### Get Dataset Split w 68/32 Split for All/Ulu2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae6844f-cb61-4d1c-88c0-4acbea558ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ulu22 vals (tr, va, te): [1037, 296, 143], total: 1476\n",
      "ulu vals (tr, va, te): [669, 191, 95], total: 955\n",
      "kk vals (tr, va, te): [1348, 384, 192], total: 1924\n",
      "cb vals (tr, va, te): [133, 38, 19], total: 190\n"
     ]
    }
   ],
   "source": [
    "pos_folder = r'E:\\tests\\1sec-manual\\inputs\\annots\\pos'\n",
    "\n",
    "# Get list of all csv files in that folder\n",
    "files_pos = glob.glob(pos_folder + \"/*.csv\")\n",
    "\n",
    "site_names = []\n",
    "num_annots = []\n",
    "\n",
    "# For each csv file\n",
    "for file in files_pos:\n",
    "\n",
    "    annots = pd.read_csv(file)\n",
    "\n",
    "    site_name = file.split('\\\\')[-1].split('.')[0].split('_')[0]\n",
    "\n",
    "    site_names.append(site_name)\n",
    "    num_annots.append(len(annots))\n",
    "\n",
    "ULU22_val = num_annots[site_names.index('ULU2022')]\n",
    "other_val = sum(num_annots) - ULU22_val\n",
    "all_annots = sum(num_annots)\n",
    "\n",
    "ulu_2022_split = 0.32\n",
    "all_else_split = 1 - ulu_2022_split\n",
    "\n",
    "dataset_split = [0.7, 0.2, 0.1]\n",
    "\n",
    "train_annots = round(all_annots*dataset_split[0])\n",
    "val_annots = round(all_annots*dataset_split[1])\n",
    "test_annots = round(all_annots*dataset_split[2])\n",
    "\n",
    "# ulu22 vals\n",
    "ulu22_tr = round(train_annots*ulu_2022_split)\n",
    "ulu22_va = round(val_annots*ulu_2022_split)\n",
    "ulu22_te = round(test_annots*ulu_2022_split)\n",
    "\n",
    "ulu22_leftovers = ULU22_val - ulu22_tr - ulu22_va - ulu22_te\n",
    "\n",
    "if ulu22_leftovers < 0:\n",
    "    ulu22_te = ulu22_te + ulu22_leftovers\n",
    "\n",
    "if ulu22_leftovers > 0 :\n",
    "    ulu22_tr = ulu22_tr + ulu22_leftovers\n",
    "\n",
    "all_ulu = ulu22_tr + ulu22_te + ulu22_va\n",
    "\n",
    "if all_ulu != ULU22_val:\n",
    "    print('Something went wrong with Ulu')\n",
    "    exit()\n",
    "\n",
    "ulu2022_vals = [ulu22_tr, ulu22_va, ulu22_te]\n",
    "\n",
    "# rest vals\n",
    "rest_tr = round(train_annots*all_else_split)\n",
    "rest_va = round(val_annots*all_else_split)\n",
    "rest_te = round(test_annots*all_else_split)\n",
    "\n",
    "# totals\n",
    "all_added = rest_tr + rest_va + rest_te\n",
    "\n",
    "if all_added < other_val:\n",
    "    leftover = other_val - all_added\n",
    "    rest_tr = rest_tr + leftover\n",
    "\n",
    "if all_added > other_val:\n",
    "    leftover = all_added - other_val\n",
    "    rest_va = rest_va - leftover\n",
    "\n",
    "all_added2 = rest_tr + rest_va + rest_te\n",
    "\n",
    "cb_perc = num_annots[0]/all_added2\n",
    "kk_perc = num_annots[1]/all_added2\n",
    "ulu_perc = num_annots[4]/all_added2\n",
    "\n",
    "# split into other site vals\n",
    "cb_tr = round(cb_perc*rest_tr)\n",
    "cb_va = round(cb_perc*rest_va)\n",
    "cb_te = round(cb_perc*rest_te)\n",
    "total_cb = cb_tr + cb_va + cb_te\n",
    "if total_cb < num_annots[0]:\n",
    "    leftover_cb = num_annots[0] - total_cb\n",
    "    cb_tr = cb_tr + leftover_cb\n",
    "if total_cb > num_annots[0]:\n",
    "    leftover_cb = num_annots[0] - total_cb\n",
    "    cb_va = cb_va + leftover_cb\n",
    "cb_vals = [cb_tr, cb_va, cb_te]\n",
    "\n",
    "kk_tr = round(kk_perc*rest_tr)\n",
    "kk_va = round(kk_perc*rest_va)\n",
    "kk_te = round(kk_perc*rest_te)\n",
    "total_kk = kk_tr + kk_va + kk_te\n",
    "if total_kk < num_annots[1]:\n",
    "    leftover_kk = num_annots[1] - total_kk\n",
    "    kk_tr = kk_tr + leftover_kk\n",
    "if total_kk > num_annots[1]:\n",
    "    leftover_kk = num_annots[1] - total_kk\n",
    "    kk_va = kk_va + leftover_kk\n",
    "kk_vals = [kk_tr, kk_va, kk_te]\n",
    "    \n",
    "ulu_tr = round(ulu_perc*rest_tr)\n",
    "ulu_va = round(ulu_perc*rest_va)\n",
    "ulu_te = round(ulu_perc*rest_te)\n",
    "total_ulu = ulu_tr + ulu_va + ulu_te\n",
    "if total_ulu < num_annots[4]:\n",
    "    leftover_ulu = num_annots[4] - total_ulu\n",
    "    ulu_tr = ulu_tr + leftover_ulu\n",
    "if total_ulu > num_annots[4]:\n",
    "    leftover_ulu = num_annots[4] - total_ulu\n",
    "    ulu_va = ulu_va + leftover_ulu\n",
    "ulu_vals = [ulu_tr, ulu_va, ulu_te]\n",
    "\n",
    "print('ulu22 vals (tr, va, te): ' + str(ulu2022_vals) + ', total: ' + str(sum(ulu2022_vals)))\n",
    "print('ulu vals (tr, va, te): ' + str(ulu_vals) + ', total: ' + str(sum(ulu_vals)))\n",
    "print('kk vals (tr, va, te): ' + str(kk_vals) + ', total: ' + str(sum(kk_vals)))\n",
    "print('cb vals (tr, va, te): ' + str(cb_vals) + ', total: ' + str(sum(cb_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "710e3ea6-899d-4c66-8300-733db26da5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in user inputs \n",
    "main_folder = r'E:\\tests\\1sec-manual'\n",
    "\n",
    "# These are copied from the 2sec edited folder \n",
    "neg_folder = r'E:\\tests\\1sec-manual\\inputs\\annots\\neg'\n",
    "pos_folder = r'E:\\tests\\1sec-manual\\inputs\\annots\\pos'\n",
    "\n",
    "file_durations_file = r'E:\\tests\\all_file_durations_complete.xlsx'\n",
    "file_durations = pd.read_excel(file_durations_file)\n",
    "\n",
    "data_folder = r'D:\\ringed-seal-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53b7698a-88cb-45fd-b029-d38bc387ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives standardized? True\n",
      "Negatives standardized? True\n",
      "Negatives standardized? True\n",
      "Negatives standardized? True\n",
      "Positives standardized? True\n",
      "Positives standardized? True\n",
      "Positives standardized? True\n",
      "Positives standardized? True\n"
     ]
    }
   ],
   "source": [
    "## Create Database ##\n",
    "\n",
    "# negatives tables and standarize for ketos\n",
    "ulu_neg = pd.read_excel(neg_folder + '\\\\' + 'ULU-negs-joined.xlsx')\n",
    "ulu_neg = ulu_neg.ffill()\n",
    "ulu_neg = sl.standardize(table=ulu_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(ulu_neg)))\n",
    "\n",
    "ulu2022_neg = pd.read_excel(neg_folder + '\\\\' + 'ULU2022-negs-joined.xlsx')\n",
    "ulu2022_neg = ulu2022_neg.ffill()\n",
    "ulu2022_neg = sl.standardize(table=ulu2022_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(ulu2022_neg)))\n",
    "\n",
    "kk_neg = pd.read_excel(neg_folder + '\\\\' + 'KK-negs-joined.xlsx')\n",
    "kk_neg = kk_neg.ffill()\n",
    "kk_neg = sl.standardize(table=kk_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(kk_neg)))\n",
    "\n",
    "cb_neg = pd.read_excel(neg_folder + '\\\\' + 'CB-negs-joined.xlsx')\n",
    "cb_neg = cb_neg.ffill()\n",
    "cb_neg = sl.standardize(table=cb_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(cb_neg)))\n",
    "\n",
    "# positives tables\n",
    "ulu_pos = pd.read_csv(pos_folder + '\\\\' + 'ULU_all_formatted_1sec.csv')\n",
    "ulu_pos = ulu_pos.ffill()\n",
    "ulu_pos = sl.standardize(table=ulu_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(ulu_pos)))\n",
    "\n",
    "ulu2022_pos = pd.read_csv(pos_folder + '\\\\' + 'ULU2022_all_formatted_1sec.csv')\n",
    "ulu2022_pos = ulu2022_pos.ffill()\n",
    "ulu2022_pos = sl.standardize(table=ulu2022_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(ulu2022_pos)))\n",
    "\n",
    "kk_pos = pd.read_csv(pos_folder + '\\\\' + 'KK_all_formatted_1sec.csv')\n",
    "kk_pos = kk_pos.ffill()\n",
    "kk_pos = sl.standardize(table=kk_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(kk_pos)))\n",
    "\n",
    "cb_pos = pd.read_csv(pos_folder + '\\\\' + 'CB_all_formatted_1sec.csv')\n",
    "cb_pos = cb_pos.ffill()\n",
    "cb_pos = sl.standardize(table=cb_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(cb_pos)))\n",
    "\n",
    "# join into complete tables\n",
    "\n",
    "ulu_pos_tr = ulu_pos.head(ulu_vals[0])\n",
    "ulu_pos_va = ulu_pos[ulu_vals[0]:ulu_vals[0] + ulu_vals[1]]\n",
    "ulu_pos_te = ulu_pos.tail(ulu_vals[2])\n",
    "\n",
    "ulu_neg_tr = ulu_neg.head(ulu_vals[0])\n",
    "ulu_neg_va = ulu_neg[ulu_vals[0]:ulu_vals[0] + ulu_vals[1]]\n",
    "ulu_neg_te = ulu_neg.tail(ulu_vals[2])\n",
    "\n",
    "ulu_tr = pd.concat([ulu_pos_tr, ulu_neg_tr])\n",
    "ulu_va = pd.concat([ulu_pos_va, ulu_neg_va])\n",
    "ulu_te = pd.concat([ulu_pos_te, ulu_neg_te])\n",
    "\n",
    "ulu2022_pos_tr = ulu2022_pos.head(ulu2022_vals[0])\n",
    "ulu2022_pos_va = ulu2022_pos[ulu2022_vals[0]:ulu2022_vals[0] + ulu2022_vals[1]]\n",
    "ulu2022_pos_te = ulu2022_pos.tail(ulu2022_vals[2])\n",
    "\n",
    "ulu2022_neg_tr = ulu2022_neg.head(ulu2022_vals[0])\n",
    "ulu2022_neg_va = ulu2022_neg[ulu2022_vals[0]:ulu2022_vals[0] + ulu2022_vals[1]]\n",
    "ulu2022_neg_te = ulu2022_neg.tail(ulu2022_vals[2])\n",
    "\n",
    "ulu2022_tr = pd.concat([ulu2022_pos_tr, ulu2022_neg_tr])\n",
    "ulu2022_va = pd.concat([ulu2022_pos_va, ulu2022_neg_va])\n",
    "ulu2022_te = pd.concat([ulu2022_pos_te, ulu2022_neg_te])\n",
    "\n",
    "kk_pos_tr = kk_pos.head(kk_vals[0])\n",
    "kk_pos_va = kk_pos[kk_vals[0]:kk_vals[0] + kk_vals[1]]\n",
    "kk_pos_te = kk_pos.tail(kk_vals[2])\n",
    "\n",
    "kk_neg_tr = kk_neg.head(kk_vals[0])\n",
    "kk_neg_va = kk_neg[kk_vals[0]:kk_vals[0] + kk_vals[1]]\n",
    "kk_neg_te = kk_neg.tail(kk_vals[2])\n",
    "\n",
    "kk_tr = pd.concat([kk_pos_tr, kk_neg_tr])\n",
    "kk_va = pd.concat([kk_pos_va, kk_neg_va])\n",
    "kk_te = pd.concat([kk_pos_te, kk_neg_te])\n",
    "\n",
    "cb_pos_tr = cb_pos.head(cb_vals[0])\n",
    "cb_pos_va = cb_pos[cb_vals[0]:cb_vals[0] + cb_vals[1]]\n",
    "cb_pos_te = cb_pos.tail(cb_vals[2])\n",
    "\n",
    "cb_neg_tr = cb_neg.head(cb_vals[0])\n",
    "cb_neg_va = cb_neg[cb_vals[0]:cb_vals[0] + cb_vals[1]]\n",
    "cb_neg_te = cb_neg.tail(cb_vals[2])\n",
    "\n",
    "cb_tr = pd.concat([cb_pos_tr, cb_neg_tr])\n",
    "cb_va = pd.concat([cb_pos_va, cb_neg_va])\n",
    "cb_te = pd.concat([cb_pos_te, cb_neg_te])\n",
    "\n",
    "# final three tables\n",
    "\n",
    "train = pd.concat([ulu_tr, ulu2022_tr, cb_tr, kk_tr])\n",
    "val = pd.concat([ulu_va, ulu2022_va, cb_va, kk_va])\n",
    "test = pd.concat([ulu_te, ulu2022_te, cb_te, kk_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ae3b56-60c7-4daf-8e4b-a1fcfd68f080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning training table of original length 6374\n",
      "Number of dropped rows: 10\n",
      "6362\n"
     ]
    }
   ],
   "source": [
    "drop_rows_tr = []\n",
    "\n",
    "print('cleaning training table of original length ' + str(len(train)))\n",
    "\n",
    "for idex, row in train.iterrows():\n",
    "\n",
    "    # filename is row[0], end time is idex.end\n",
    "    index = file_durations.loc[file_durations['filename'] == row.name[0]].index\n",
    "    duration = file_durations['duration'][index].values[0]\n",
    "\n",
    "    if duration < row.end:\n",
    "        # drop the row corresponding to that sel_id and filename from the dataframe\n",
    "        drop_rows_tr.append(idex)\n",
    "\n",
    "    if row.start < 0:\n",
    "        drop_rows_tr.append(idex)\n",
    "\n",
    "print('Number of dropped rows: ' + str(len(drop_rows_tr)))\n",
    "\n",
    "train = train.drop(drop_rows_tr)\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd2a51e-2711-465a-8cc1-de11e4cac861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning validation table of original length 1818\n",
      "Number of dropped rows: 4\n",
      "1813\n"
     ]
    }
   ],
   "source": [
    "drop_rows_va = []\n",
    "\n",
    "print('cleaning validation table of original length ' + str(len(val)))\n",
    "\n",
    "for idex, row in val.iterrows():\n",
    "\n",
    "    # filename is row[0], end time is idex.end\n",
    "    index = file_durations.loc[file_durations['filename'] == row.name[0]].index\n",
    "    duration = file_durations['duration'][index].values[0]\n",
    "\n",
    "    if duration < row.end:\n",
    "        # drop the row corresponding to that sel_id and filename from the dataframe\n",
    "        drop_rows_va.append(idex)\n",
    "\n",
    "    if row.start < 0:\n",
    "        drop_rows_va.append(idex)\n",
    "\n",
    "print('Number of dropped rows: ' + str(len(drop_rows_va)))\n",
    "\n",
    "val = val.drop(drop_rows_va)\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dc7cde1-4b67-4b80-9cee-af1e6aef8964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning test table of original length 898\n",
      "Number of dropped rows: 2\n",
      "894\n"
     ]
    }
   ],
   "source": [
    "drop_rows_te = []\n",
    "\n",
    "print('cleaning test table of original length ' + str(len(test)))\n",
    "\n",
    "for idex, row in test.iterrows():\n",
    "\n",
    "    # filename is row[0], end time is idex.end\n",
    "    index = file_durations.loc[file_durations['filename'] == row.name[0]].index\n",
    "    duration = file_durations['duration'][index].values[0]\n",
    "\n",
    "    if duration < row.end:\n",
    "        # drop the row corresponding to that sel_id and filename from the dataframe\n",
    "        drop_rows_te.append(idex)\n",
    "\n",
    "    if row.start < 0:\n",
    "        drop_rows_te.append(idex)\n",
    "\n",
    "print('Number of dropped rows: ' + str(len(drop_rows_te)))\n",
    "\n",
    "test = test.drop(drop_rows_te)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1176cf-3b4b-4601-9208-25dd64012361",
   "metadata": {},
   "source": [
    "### Spectro Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57270aa1-c0f2-4d26-b97d-0e270280905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of spectrogram tests \n",
    "spectro_folder = r'E:\\tests\\spectro-files'\n",
    "spectro_files = glob.glob(spectro_folder + \"/*.json\")\n",
    "\n",
    "recipe = r'E:\\tests\\recipe-files\\resnet_recipe-1.json'\n",
    "\n",
    "spectro_test_folder = r'E:\\tests\\1sec-manual\\spectro-tests'\n",
    "\n",
    "# one: 100 to 1200 (usual)\n",
    "# two: 0 to 1500\n",
    "# three: 0 to 3000\n",
    "# four: usual, rate 2000\n",
    "# five: usual, rate 4000\n",
    "# six: usual, window 0.032\n",
    "\n",
    "db_names = [spectro_test_folder + '\\\\' + 'spectro-1-db.h5', spectro_test_folder + '\\\\' + 'spectro-2-db.h5', spectro_test_folder + '\\\\' + 'spectro-3-db.h5',\n",
    "             spectro_test_folder + '\\\\' + 'spectro-4-db.h5', spectro_test_folder + '\\\\' + 'spectro-5-db.h5', spectro_test_folder + '\\\\' + 'spectro-6-db.h5']\n",
    "\n",
    "model_names = [spectro_test_folder + '\\\\' + 'rs-spec1.kt', spectro_test_folder + '\\\\' + 'rs-spec2.kt', spectro_test_folder + '\\\\' + 'rs-spec3.kt',\n",
    "               spectro_test_folder + '\\\\' + 'rs-spec4.kt', spectro_test_folder + '\\\\' + 'rs-spec5.kt', spectro_test_folder + '\\\\' + 'rs-spec6.kt']\n",
    "\n",
    "temp_folders = [spectro_test_folder + '\\\\' + 'rs-temp-1', spectro_test_folder + '\\\\' + 'rs-temp-2', spectro_test_folder + '\\\\' + 'rs-temp-3',\n",
    "               spectro_test_folder + '\\\\' + 'rs-temp-4', spectro_test_folder + '\\\\' + 'rs-temp-5', spectro_test_folder + '\\\\' + 'rs-temp-6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17bc68ee-d3c2-4916-a28a-7413225623f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tables\n",
    "#tables.file._open_files.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a230988-fe7c-416c-8d9e-2566471a7c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6362/6362 [02:33<00:00, 41.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6362 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-1-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1813/1813 [00:42<00:00, 42.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-1-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 894/894 [00:21<00:00, 41.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-1-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6362/6362 [01:06<00:00, 95.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6362 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-2-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1813/1813 [00:19<00:00, 94.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-2-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 894/894 [00:09<00:00, 94.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-2-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6362/6362 [01:07<00:00, 93.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6362 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-3-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1813/1813 [00:19<00:00, 92.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-3-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 894/894 [00:09<00:00, 90.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-3-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6362/6362 [01:09<00:00, 92.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6362 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-4-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1813/1813 [00:19<00:00, 91.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-4-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 894/894 [00:09<00:00, 92.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-4-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6362/6362 [01:36<00:00, 65.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6362 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-5-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1813/1813 [00:43<00:00, 41.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-5-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 894/894 [00:23<00:00, 37.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-5-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6362/6362 [02:12<00:00, 48.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6362 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-6-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1813/1813 [00:35<00:00, 51.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-6-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 894/894 [00:15<00:00, 58.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894 items saved to E:\\tests\\1sec-manual\\spectro-tests\\spectro-6-db.h5\n"
     ]
    }
   ],
   "source": [
    "# For each spectro file:\n",
    "for idex, spec in enumerate(spectro_files):\n",
    "\n",
    "    spec_file = spectro_folder + '\\\\' + 'spec_config_' + str(idex+1) + '.json'\n",
    "    \n",
    "    # join into a database\n",
    "    # Load the spectrogram representation & parameters, this returns a dict \n",
    "    spec_cfg = load_audio_representation(spec_file, name=\"spectrogram\")\n",
    "\n",
    "    # Create a table called \"train\" in the database, defined by db_name, using the \"train\" selections table, the spectrogram config, and the audio data \n",
    "    # Behind the hood, this creates an AudioLoader and AudioWriter Ketos function which generates the spectrograms for each selection \n",
    "    # For the specific spectrogram in this case, the spectrograms are of size [1500,56] where 56 refers to the frequency dimension and 1500 refers to the time dimension \n",
    "    # The size of the spectrogram is 1500*56, which is 84000\n",
    "    dbi.create_database(output_file=db_names[idex],  # empty brackets\n",
    "                        dataset_name=r'train', selections=train, data_dir=data_folder,\n",
    "                        audio_repres=spec_cfg)\n",
    "    \n",
    "    dbi.create_database(output_file=db_names[idex],  # empty brackets\n",
    "                        dataset_name=r'val', selections=val, data_dir=data_folder,\n",
    "                        audio_repres=spec_cfg)\n",
    "    \n",
    "    dbi.create_database(output_file=db_names[idex],  # empty brackets\n",
    "                        dataset_name=r'test', selections=test, data_dir=data_folder,\n",
    "                        audio_repres=spec_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de4066fd-ec4d-4281-869a-b63f95438d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done spectro 1\n",
      "Done spectro 2\n",
      "Done spectro 3\n",
      "Done spectro 4\n",
      "Done spectro 5\n",
      "Done spectro 6\n"
     ]
    }
   ],
   "source": [
    "for idex, spec in enumerate(spectro_files): \n",
    "\n",
    "    spec_file = spectro_folder + '\\\\' + 'spec_config_' + str(idex+1) + '.json'\n",
    "    \n",
    "    # join into a database\n",
    "    # Load the spectrogram representation & parameters, this returns a dict \n",
    "    spec_cfg = load_audio_representation(spec_file, name=\"spectrogram\")\n",
    "\n",
    "    # Set the random seed for numpy and tensorflow \n",
    "    np.random.seed(1000)\n",
    "    tf.random.set_seed(2000)\n",
    "    \n",
    "    # Set the batch size and number of epochs for training\n",
    "    batch_size = 16\n",
    "    n_epochs = 40\n",
    "    \n",
    "    # Set the log folder and checkpoint folder \n",
    "    log_folder = spectro_test_folder + '\\\\' + 'logs' + str(idex)\n",
    "    checkpoint_folder = spectro_test_folder + '\\\\' + 'checkpoints' + str(idex)\n",
    "    \n",
    "    # Open the database file in read mode\n",
    "    db = dbi.open_file(db_names[idex], 'r')\n",
    "    \n",
    "    # Open the training and validation tables respectively \n",
    "    train_data = dbi.open_table(db, \"/train/data\")\n",
    "    val_data = dbi.open_table(db, \"/val/data\")\n",
    "    \n",
    "    # Create batches of training data of size batch size, using the specified data table \n",
    "    # This returns indices of the data in each batch along with their labels \n",
    "    train_generator = BatchGenerator(batch_size=batch_size, data_table=train_data,\n",
    "                                        output_transform_func=ResNetInterface.transform_batch,\n",
    "                                        shuffle=True, refresh_on_epoch_end=True)\n",
    "    \n",
    "    # Create batches of validation data of size batch size, using the specified data table \n",
    "    # This returns indices of the data in each batch along with their labels \n",
    "    val_generator = BatchGenerator(batch_size=batch_size, data_table=val_data,\n",
    "                                       output_transform_func=ResNetInterface.transform_batch,\n",
    "                                       shuffle=False, refresh_on_epoch_end=False)\n",
    "    \n",
    "    # Build the ResNet model file based off of the recipe file - this creates a \"ResNetInterface\" object \n",
    "    resnet = ResNetInterface.build(recipe)\n",
    "    \n",
    "    # Set the training and validation generators to the batch generators created above \n",
    "    resnet.train_generator = train_generator\n",
    "    resnet.val_generator = val_generator\n",
    "    \n",
    "    # Set the model log and checkpoint directory \n",
    "    resnet.log_dir = log_folder\n",
    "    resnet.checkpoint_dir = checkpoint_folder\n",
    "    \n",
    "    # Train the model, looping through all of the training and validation data \n",
    "    # See code map for more information\n",
    "    resnet.train_loop(n_epochs=n_epochs, verbose=False, log_csv=True, csv_name='log.csv')\n",
    "    \n",
    "    # Close the database \n",
    "    db.close()\n",
    "    \n",
    "    # Save the model file, and keep track of the spectrogram parameters used to generate that model \n",
    "    resnet.save(model_names[idex], audio_repr_file=spec_file)\n",
    "\n",
    "    print('Done spectro ' + str(idex+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12de46-58cc-4279-8e61-bf66cd7d6185",
   "metadata": {},
   "source": [
    "#### Recipe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9a9ab7b-1717-46f4-80ab-2c53fe97bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of spectrogram tests \n",
    "spectro_folder = r'E:\\tests\\spectro-files'\n",
    "spectro_files = glob.glob(spectro_folder + \"/*.json\")\n",
    "\n",
    "recipe = r'E:\\tests\\recipe-files\\resnet_recipe-2.json'\n",
    "\n",
    "spectro_test_folder = r'E:\\tests\\1sec-manual\\recipe-2'\n",
    "\n",
    "# one: 100 to 1200 (usual)\n",
    "# two: 0 to 1500\n",
    "# three: 0 to 3000\n",
    "# four: usual, rate 2000\n",
    "# five: usual, rate 4000\n",
    "# six: usual, window 0.032\n",
    "\n",
    "db_names = [r'E:\\tests\\1sec-manual\\spectro-tests' + '\\\\' + 'spectro-1-db.h5', r'E:\\tests\\1sec-manual\\spectro-tests' + '\\\\' + 'spectro-2-db.h5', \n",
    "            r'E:\\tests\\1sec-manual\\spectro-tests' + '\\\\' + 'spectro-3-db.h5', r'E:\\tests\\1sec-manual\\spectro-tests' + '\\\\' + 'spectro-4-db.h5', \n",
    "            r'E:\\tests\\1sec-manual\\spectro-tests' + '\\\\' + 'spectro-5-db.h5', r'E:\\tests\\1sec-manual\\spectro-tests' + '\\\\' + 'spectro-6-db.h5']\n",
    "\n",
    "model_names = [spectro_test_folder + '\\\\' + 'rs-spec1-r2.kt', spectro_test_folder + '\\\\' + 'rs-spec2-r2.kt', spectro_test_folder + '\\\\' + 'rs-spec3-r2.kt',\n",
    "               spectro_test_folder + '\\\\' + 'rs-spec4-r2.kt', spectro_test_folder + '\\\\' + 'rs-spec5-r2.kt', spectro_test_folder + '\\\\' + 'rs-spec6-r2.kt']\n",
    "\n",
    "temp_folders = [spectro_test_folder + '\\\\' + 'rs-temp-1', spectro_test_folder + '\\\\' + 'rs-temp-2', spectro_test_folder + '\\\\' + 'rs-temp-3',\n",
    "               spectro_test_folder + '\\\\' + 'rs-temp-4', spectro_test_folder + '\\\\' + 'rs-temp-5', spectro_test_folder + '\\\\' + 'rs-temp-6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc666416-e791-4c01-93e0-c8e0ed0fe483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done spectro 1\n",
      "Done spectro 2\n",
      "Done spectro 3\n",
      "Done spectro 4\n",
      "Done spectro 5\n",
      "Done spectro 6\n"
     ]
    }
   ],
   "source": [
    "for idex, spec in enumerate(spectro_files): \n",
    "\n",
    "    spec_file = spectro_folder + '\\\\' + 'spec_config_' + str(idex+1) + '.json'\n",
    "    \n",
    "    # join into a database\n",
    "    # Load the spectrogram representation & parameters, this returns a dict \n",
    "    spec_cfg = load_audio_representation(spec_file, name=\"spectrogram\")\n",
    "\n",
    "    # Set the random seed for numpy and tensorflow \n",
    "    np.random.seed(1000)\n",
    "    tf.random.set_seed(2000)\n",
    "    \n",
    "    # Set the batch size and number of epochs for training\n",
    "    batch_size = 16\n",
    "    n_epochs = 40\n",
    "    \n",
    "    # Set the log folder and checkpoint folder \n",
    "    log_folder = spectro_test_folder + '\\\\' + 'logs' + str(idex)\n",
    "    checkpoint_folder = spectro_test_folder + '\\\\' + 'checkpoints' + str(idex)\n",
    "    \n",
    "    # Open the database file in read mode\n",
    "    db = dbi.open_file(db_names[idex], 'r')\n",
    "    \n",
    "    # Open the training and validation tables respectively \n",
    "    train_data = dbi.open_table(db, \"/train/data\")\n",
    "    val_data = dbi.open_table(db, \"/val/data\")\n",
    "    \n",
    "    # Create batches of training data of size batch size, using the specified data table \n",
    "    # This returns indices of the data in each batch along with their labels \n",
    "    train_generator = BatchGenerator(batch_size=batch_size, data_table=train_data,\n",
    "                                        output_transform_func=ResNetInterface.transform_batch,\n",
    "                                        shuffle=True, refresh_on_epoch_end=True)\n",
    "    \n",
    "    # Create batches of validation data of size batch size, using the specified data table \n",
    "    # This returns indices of the data in each batch along with their labels \n",
    "    val_generator = BatchGenerator(batch_size=batch_size, data_table=val_data,\n",
    "                                       output_transform_func=ResNetInterface.transform_batch,\n",
    "                                       shuffle=False, refresh_on_epoch_end=False)\n",
    "    \n",
    "    # Build the ResNet model file based off of the recipe file - this creates a \"ResNetInterface\" object \n",
    "    resnet = ResNetInterface.build(recipe)\n",
    "    \n",
    "    # Set the training and validation generators to the batch generators created above \n",
    "    resnet.train_generator = train_generator\n",
    "    resnet.val_generator = val_generator\n",
    "    \n",
    "    # Set the model log and checkpoint directory \n",
    "    resnet.log_dir = log_folder\n",
    "    resnet.checkpoint_dir = checkpoint_folder\n",
    "    \n",
    "    # Train the model, looping through all of the training and validation data \n",
    "    # See code map for more information\n",
    "    resnet.train_loop(n_epochs=n_epochs, verbose=False, log_csv=True, csv_name='log.csv')\n",
    "    \n",
    "    # Close the database \n",
    "    db.close()\n",
    "    \n",
    "    # Save the model file, and keep track of the spectrogram parameters used to generate that model \n",
    "    resnet.save(model_names[idex], audio_repr_file=spec_file)\n",
    "\n",
    "    print('Done spectro ' + str(idex+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dec1d7-b341-458b-9c0f-0f2326af27e4",
   "metadata": {},
   "source": [
    "# Auto Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ec4a5c5-a5ae-4dd8-bd8a-dc5a6f02c026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ulu22 vals (tr, va, te): [2266, 647, 262], total: 3175\n",
      "ulu vals (tr, va, te): [1568, 442, 221], total: 2231\n",
      "kk vals (tr, va, te): [2874, 811, 405], total: 4090\n",
      "cb vals (tr, va, te): [294, 83, 41], total: 418\n"
     ]
    }
   ],
   "source": [
    "pos_folder = r'E:\\tests\\1sec-auto\\inputs\\annots\\pos'\n",
    "\n",
    "# Get list of all csv files in that folder\n",
    "files_pos = glob.glob(pos_folder + \"/*.csv\")\n",
    "\n",
    "site_names = []\n",
    "num_annots = []\n",
    "\n",
    "# For each csv file\n",
    "for file in files_pos:\n",
    "\n",
    "    annots = pd.read_csv(file)\n",
    "\n",
    "    site_name = file.split('\\\\')[-1].split('.')[0].split('_')[0]\n",
    "\n",
    "    site_names.append(site_name)\n",
    "    num_annots.append(len(annots))\n",
    "\n",
    "ULU22_val = num_annots[site_names.index('ULU2022')]\n",
    "other_val = sum(num_annots) - ULU22_val\n",
    "all_annots = sum(num_annots)\n",
    "\n",
    "ulu_2022_split = 0.32\n",
    "all_else_split = 1 - ulu_2022_split\n",
    "\n",
    "dataset_split = [0.7, 0.2, 0.1]\n",
    "\n",
    "train_annots = round(all_annots*dataset_split[0])\n",
    "val_annots = round(all_annots*dataset_split[1])\n",
    "test_annots = round(all_annots*dataset_split[2])\n",
    "\n",
    "# ulu22 vals\n",
    "ulu22_tr = round(train_annots*ulu_2022_split)\n",
    "ulu22_va = round(val_annots*ulu_2022_split)\n",
    "ulu22_te = round(test_annots*ulu_2022_split)\n",
    "\n",
    "ulu22_leftovers = ULU22_val - ulu22_tr - ulu22_va - ulu22_te\n",
    "\n",
    "if ulu22_leftovers < 0:\n",
    "    ulu22_te = ulu22_te + ulu22_leftovers\n",
    "\n",
    "if ulu22_leftovers > 0 :\n",
    "    ulu22_tr = ulu22_tr + ulu22_leftovers\n",
    "\n",
    "all_ulu = ulu22_tr + ulu22_te + ulu22_va\n",
    "\n",
    "if all_ulu != ULU22_val:\n",
    "    print('Something went wrong with Ulu')\n",
    "    exit()\n",
    "\n",
    "ulu2022_vals = [ulu22_tr, ulu22_va, ulu22_te]\n",
    "\n",
    "# rest vals\n",
    "rest_tr = round(train_annots*all_else_split)\n",
    "rest_va = round(val_annots*all_else_split)\n",
    "rest_te = round(test_annots*all_else_split)\n",
    "\n",
    "# totals\n",
    "all_added = rest_tr + rest_va + rest_te\n",
    "\n",
    "if all_added < other_val:\n",
    "    leftover = other_val - all_added\n",
    "    rest_tr = rest_tr + leftover\n",
    "\n",
    "if all_added > other_val:\n",
    "    leftover = all_added - other_val\n",
    "    rest_va = rest_va - leftover\n",
    "\n",
    "all_added2 = rest_tr + rest_va + rest_te\n",
    "\n",
    "cb_perc = num_annots[0]/all_added2\n",
    "kk_perc = num_annots[1]/all_added2\n",
    "ulu_perc = num_annots[4]/all_added2\n",
    "\n",
    "# split into other site vals\n",
    "cb_tr = round(cb_perc*rest_tr)\n",
    "cb_va = round(cb_perc*rest_va)\n",
    "cb_te = round(cb_perc*rest_te)\n",
    "total_cb = cb_tr + cb_va + cb_te\n",
    "if total_cb < num_annots[0]:\n",
    "    leftover_cb = num_annots[0] - total_cb\n",
    "    cb_tr = cb_tr + leftover_cb\n",
    "if total_cb > num_annots[0]:\n",
    "    leftover_cb = num_annots[0] - total_cb\n",
    "    cb_va = cb_va + leftover_cb\n",
    "cb_vals = [cb_tr, cb_va, cb_te]\n",
    "\n",
    "kk_tr = round(kk_perc*rest_tr)\n",
    "kk_va = round(kk_perc*rest_va)\n",
    "kk_te = round(kk_perc*rest_te)\n",
    "total_kk = kk_tr + kk_va + kk_te\n",
    "if total_kk < num_annots[1]:\n",
    "    leftover_kk = num_annots[1] - total_kk\n",
    "    kk_tr = kk_tr + leftover_kk\n",
    "if total_kk > num_annots[1]:\n",
    "    leftover_kk = num_annots[1] - total_kk\n",
    "    kk_va = kk_va + leftover_kk\n",
    "kk_vals = [kk_tr, kk_va, kk_te]\n",
    "    \n",
    "ulu_tr = round(ulu_perc*rest_tr)\n",
    "ulu_va = round(ulu_perc*rest_va)\n",
    "ulu_te = round(ulu_perc*rest_te)\n",
    "total_ulu = ulu_tr + ulu_va + ulu_te\n",
    "if total_ulu < num_annots[4]:\n",
    "    leftover_ulu = num_annots[4] - total_ulu\n",
    "    ulu_tr = ulu_tr + leftover_ulu\n",
    "if total_ulu > num_annots[4]:\n",
    "    leftover_ulu = num_annots[4] - total_ulu\n",
    "    ulu_va = ulu_va + leftover_ulu\n",
    "ulu_vals = [ulu_tr, ulu_va, ulu_te]\n",
    "\n",
    "print('ulu22 vals (tr, va, te): ' + str(ulu2022_vals) + ', total: ' + str(sum(ulu2022_vals)))\n",
    "print('ulu vals (tr, va, te): ' + str(ulu_vals) + ', total: ' + str(sum(ulu_vals)))\n",
    "print('kk vals (tr, va, te): ' + str(kk_vals) + ', total: ' + str(sum(kk_vals)))\n",
    "print('cb vals (tr, va, te): ' + str(cb_vals) + ', total: ' + str(sum(cb_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e5b2f17-7fb7-408a-b7bb-e98aee63378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in user inputs \n",
    "main_folder = r'E:\\tests\\1sec-auto'\n",
    "\n",
    "# These are copied from the 2sec edited folder \n",
    "neg_folder = r'E:\\tests\\1sec-auto\\inputs\\annots\\neg'\n",
    "pos_folder = r'E:\\tests\\1sec-auto\\inputs\\annots\\pos'\n",
    "\n",
    "file_durations_file = r'E:\\tests\\all_file_durations_complete.xlsx'\n",
    "file_durations = pd.read_excel(file_durations_file)\n",
    "\n",
    "data_folder = r'D:\\ringed-seal-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80519e93-bf64-41c4-8f7a-dfcde2eaa75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives standardized? True\n",
      "Negatives standardized? True\n",
      "Negatives standardized? True\n",
      "Negatives standardized? True\n",
      "Positives standardized? True\n",
      "Positives standardized? True\n",
      "Positives standardized? True\n",
      "Positives standardized? True\n"
     ]
    }
   ],
   "source": [
    "## Create Database ##\n",
    "\n",
    "# negatives tables and standarize for ketos\n",
    "ulu_neg = pd.read_csv(neg_folder + '\\\\' + 'ULU_all_formatted_shifted_negatives.csv')\n",
    "ulu_neg = ulu_neg.ffill()\n",
    "ulu_neg = sl.standardize(table=ulu_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(ulu_neg)))\n",
    "\n",
    "ulu2022_neg = pd.read_csv(neg_folder + '\\\\' + 'ULU2022_all_formatted_shifted_negatives.csv')\n",
    "ulu2022_neg = ulu2022_neg.ffill()\n",
    "ulu2022_neg = sl.standardize(table=ulu2022_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(ulu2022_neg)))\n",
    "\n",
    "kk_neg = pd.read_csv(neg_folder + '\\\\' + 'KK_all_formatted_shifted_negatives.csv')\n",
    "kk_neg = kk_neg.ffill()\n",
    "kk_neg = sl.standardize(table=kk_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(kk_neg)))\n",
    "\n",
    "cb_neg = pd.read_csv(neg_folder + '\\\\' + 'CB_all_formatted_shifted_negatives.csv')\n",
    "cb_neg = cb_neg.ffill()\n",
    "cb_neg = sl.standardize(table=cb_neg)\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(cb_neg)))\n",
    "\n",
    "# positives tables\n",
    "ulu_pos = pd.read_csv(pos_folder + '\\\\' + 'ULU_all_formatted_shifted.csv')\n",
    "ulu_pos = ulu_pos.ffill()\n",
    "ulu_pos = sl.standardize(table=ulu_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(ulu_pos)))\n",
    "\n",
    "ulu2022_pos = pd.read_csv(pos_folder + '\\\\' + 'ULU2022_all_formatted_shifted.csv')\n",
    "ulu2022_pos = ulu2022_pos.ffill()\n",
    "ulu2022_pos = sl.standardize(table=ulu2022_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(ulu2022_pos)))\n",
    "\n",
    "kk_pos = pd.read_csv(pos_folder + '\\\\' + 'KK_all_formatted_shifted.csv')\n",
    "kk_pos = kk_pos.ffill()\n",
    "kk_pos = sl.standardize(table=kk_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(kk_pos)))\n",
    "\n",
    "cb_pos = pd.read_csv(pos_folder + '\\\\' + 'CB_all_formatted_shifted.csv')\n",
    "cb_pos = cb_pos.ffill()\n",
    "cb_pos = sl.standardize(table=cb_pos, start_labels_at_1=True)\n",
    "print('Positives standardized? ' + str(sl.is_standardized(cb_pos)))\n",
    "\n",
    "# join into complete tables\n",
    "\n",
    "ulu_pos_tr = ulu_pos.head(ulu_vals[0])\n",
    "ulu_pos_va = ulu_pos[ulu_vals[0]:ulu_vals[0] + ulu_vals[1]]\n",
    "ulu_pos_te = ulu_pos.tail(ulu_vals[2])\n",
    "\n",
    "ulu_neg_tr = ulu_neg.head(ulu_vals[0])\n",
    "ulu_neg_va = ulu_neg[ulu_vals[0]:ulu_vals[0] + ulu_vals[1]]\n",
    "ulu_neg_te = ulu_neg.tail(ulu_vals[2])\n",
    "\n",
    "ulu_tr = pd.concat([ulu_pos_tr, ulu_neg_tr])\n",
    "ulu_va = pd.concat([ulu_pos_va, ulu_neg_va])\n",
    "ulu_te = pd.concat([ulu_pos_te, ulu_neg_te])\n",
    "\n",
    "ulu2022_pos_tr = ulu2022_pos.head(ulu2022_vals[0])\n",
    "ulu2022_pos_va = ulu2022_pos[ulu2022_vals[0]:ulu2022_vals[0] + ulu2022_vals[1]]\n",
    "ulu2022_pos_te = ulu2022_pos.tail(ulu2022_vals[2])\n",
    "\n",
    "ulu2022_neg_tr = ulu2022_neg.head(ulu2022_vals[0])\n",
    "ulu2022_neg_va = ulu2022_neg[ulu2022_vals[0]:ulu2022_vals[0] + ulu2022_vals[1]]\n",
    "ulu2022_neg_te = ulu2022_neg.tail(ulu2022_vals[2])\n",
    "\n",
    "ulu2022_tr = pd.concat([ulu2022_pos_tr, ulu2022_neg_tr])\n",
    "ulu2022_va = pd.concat([ulu2022_pos_va, ulu2022_neg_va])\n",
    "ulu2022_te = pd.concat([ulu2022_pos_te, ulu2022_neg_te])\n",
    "\n",
    "kk_pos_tr = kk_pos.head(kk_vals[0])\n",
    "kk_pos_va = kk_pos[kk_vals[0]:kk_vals[0] + kk_vals[1]]\n",
    "kk_pos_te = kk_pos.tail(kk_vals[2])\n",
    "\n",
    "kk_neg_tr = kk_neg.head(kk_vals[0])\n",
    "kk_neg_va = kk_neg[kk_vals[0]:kk_vals[0] + kk_vals[1]]\n",
    "kk_neg_te = kk_neg.tail(kk_vals[2])\n",
    "\n",
    "kk_tr = pd.concat([kk_pos_tr, kk_neg_tr])\n",
    "kk_va = pd.concat([kk_pos_va, kk_neg_va])\n",
    "kk_te = pd.concat([kk_pos_te, kk_neg_te])\n",
    "\n",
    "cb_pos_tr = cb_pos.head(cb_vals[0])\n",
    "cb_pos_va = cb_pos[cb_vals[0]:cb_vals[0] + cb_vals[1]]\n",
    "cb_pos_te = cb_pos.tail(cb_vals[2])\n",
    "\n",
    "cb_neg_tr = cb_neg.head(cb_vals[0])\n",
    "cb_neg_va = cb_neg[cb_vals[0]:cb_vals[0] + cb_vals[1]]\n",
    "cb_neg_te = cb_neg.tail(cb_vals[2])\n",
    "\n",
    "cb_tr = pd.concat([cb_pos_tr, cb_neg_tr])\n",
    "cb_va = pd.concat([cb_pos_va, cb_neg_va])\n",
    "cb_te = pd.concat([cb_pos_te, cb_neg_te])\n",
    "\n",
    "# final three tables\n",
    "\n",
    "train = pd.concat([ulu_tr, ulu2022_tr, cb_tr, kk_tr])\n",
    "val = pd.concat([ulu_va, ulu2022_va, cb_va, kk_va])\n",
    "test = pd.concat([ulu_te, ulu2022_te, cb_te, kk_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e20a9db2-efc2-4e28-8d83-c7289645c925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning training table of original length 14004\n",
      "Number of dropped rows: 21\n",
      "13969\n"
     ]
    }
   ],
   "source": [
    "drop_rows_tr = []\n",
    "\n",
    "print('cleaning training table of original length ' + str(len(train)))\n",
    "\n",
    "for idex, row in train.iterrows():\n",
    "\n",
    "    # filename is row[0], end time is idex.end\n",
    "    index = file_durations.loc[file_durations['filename'] == row.name[0]].index\n",
    "    duration = file_durations['duration'][index].values[0]\n",
    "\n",
    "    if duration < row.end:\n",
    "        # drop the row corresponding to that sel_id and filename from the dataframe\n",
    "        drop_rows_tr.append(idex)\n",
    "\n",
    "    if row.start < 0:\n",
    "        drop_rows_tr.append(idex)\n",
    "\n",
    "print('Number of dropped rows: ' + str(len(drop_rows_tr)))\n",
    "\n",
    "train = train.drop(drop_rows_tr)\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a7b359d-5b6e-49f0-9265-dae913649317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning validation table of original length 3966\n",
      "Number of dropped rows: 9\n",
      "3956\n"
     ]
    }
   ],
   "source": [
    "drop_rows_va = []\n",
    "\n",
    "print('cleaning validation table of original length ' + str(len(val)))\n",
    "\n",
    "for idex, row in val.iterrows():\n",
    "\n",
    "    # filename is row[0], end time is idex.end\n",
    "    index = file_durations.loc[file_durations['filename'] == row.name[0]].index\n",
    "    duration = file_durations['duration'][index].values[0]\n",
    "\n",
    "    if duration < row.end:\n",
    "        # drop the row corresponding to that sel_id and filename from the dataframe\n",
    "        drop_rows_va.append(idex)\n",
    "\n",
    "    if row.start < 0:\n",
    "        drop_rows_va.append(idex)\n",
    "\n",
    "print('Number of dropped rows: ' + str(len(drop_rows_va)))\n",
    "\n",
    "val = val.drop(drop_rows_va)\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18f39d24-777b-4fee-bc82-5030a5ea712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning test table of original length 1858\n",
      "Number of dropped rows: 5\n",
      "1849\n"
     ]
    }
   ],
   "source": [
    "drop_rows_te = []\n",
    "\n",
    "print('cleaning test table of original length ' + str(len(test)))\n",
    "\n",
    "for idex, row in test.iterrows():\n",
    "\n",
    "    # filename is row[0], end time is idex.end\n",
    "    index = file_durations.loc[file_durations['filename'] == row.name[0]].index\n",
    "    duration = file_durations['duration'][index].values[0]\n",
    "\n",
    "    if duration < row.end:\n",
    "        # drop the row corresponding to that sel_id and filename from the dataframe\n",
    "        drop_rows_te.append(idex)\n",
    "\n",
    "    if row.start < 0:\n",
    "        drop_rows_te.append(idex)\n",
    "\n",
    "print('Number of dropped rows: ' + str(len(drop_rows_te)))\n",
    "\n",
    "test = test.drop(drop_rows_te)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebc67956-0f71-419d-8c93-fc4182fe7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of spectrogram tests \n",
    "spectro_folder = r'E:\\tests\\spectro-files'\n",
    "spectro_files = glob.glob(spectro_folder + \"/*.json\")\n",
    "\n",
    "recipe = r'E:\\tests\\recipe-files\\resnet_recipe-1.json'\n",
    "\n",
    "spectro_test_folder = r'E:\\tests\\1sec-auto\\recipe-1'\n",
    "\n",
    "# one: 100 to 1200 (usual)\n",
    "# two: 0 to 1500\n",
    "# three: 0 to 3000\n",
    "# four: usual, rate 2000\n",
    "# five: usual, rate 4000\n",
    "# six: usual, window 0.032\n",
    "\n",
    "db_names = [spectro_test_folder + '\\\\' + 'spectro-1-db.h5', spectro_test_folder + '\\\\' + 'spectro-2-db.h5', spectro_test_folder + '\\\\' + 'spectro-3-db.h5',\n",
    "             spectro_test_folder + '\\\\' + 'spectro-4-db.h5', spectro_test_folder + '\\\\' + 'spectro-5-db.h5', spectro_test_folder + '\\\\' + 'spectro-6-db.h5']\n",
    "\n",
    "model_names = [spectro_test_folder + '\\\\' + 'rs-spec1.kt', spectro_test_folder + '\\\\' + 'rs-spec2.kt', spectro_test_folder + '\\\\' + 'rs-spec3.kt',\n",
    "               spectro_test_folder + '\\\\' + 'rs-spec4.kt', spectro_test_folder + '\\\\' + 'rs-spec5.kt', spectro_test_folder + '\\\\' + 'rs-spec6.kt']\n",
    "\n",
    "temp_folders = [spectro_test_folder + '\\\\' + 'rs-temp-1', spectro_test_folder + '\\\\' + 'rs-temp-2', spectro_test_folder + '\\\\' + 'rs-temp-3',\n",
    "               spectro_test_folder + '\\\\' + 'rs-temp-4', spectro_test_folder + '\\\\' + 'rs-temp-5', spectro_test_folder + '\\\\' + 'rs-temp-6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d2488c9-45f1-4a71-99cf-a3f4c9cc66cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████████████████████████████████████▋      | 12813/13969 [05:30<00:28, 40.74it/s]RuntimeWarning: Waveform padded with its own reflection to achieve required length to compute the stft. 20 samples were padded on the left and 0 samples were padded on the right\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 13969/13969 [06:03<00:00, 38.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13969 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-1-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3956/3956 [01:34<00:00, 41.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3956 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-1-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1849/1849 [00:45<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-1-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13969/13969 [05:56<00:00, 39.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13969 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-2-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3956/3956 [01:32<00:00, 42.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3956 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-2-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1849/1849 [00:45<00:00, 40.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-2-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13969/13969 [05:44<00:00, 40.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13969 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-3-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3956/3956 [01:36<00:00, 40.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3956 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-3-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1849/1849 [00:46<00:00, 39.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-3-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████████████████████████████████████▋      | 12813/13969 [05:00<00:31, 36.22it/s]RuntimeWarning: Waveform padded with its own reflection to achieve required length to compute the stft. 13 samples were padded on the left and 0 samples were padded on the right\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 13969/13969 [05:33<00:00, 41.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13969 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-4-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3956/3956 [01:31<00:00, 43.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3956 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-4-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1849/1849 [00:43<00:00, 42.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-4-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████████████████████████████████████▋      | 12811/13969 [06:18<00:38, 30.22it/s]RuntimeWarning: Waveform padded with its own reflection to achieve required length to compute the stft. 27 samples were padded on the left and 0 samples were padded on the right\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 13969/13969 [06:58<00:00, 33.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13969 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-5-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3956/3956 [01:57<00:00, 33.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3956 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-5-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1849/1849 [00:55<00:00, 33.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-5-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13969/13969 [05:18<00:00, 43.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13969 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-6-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3956/3956 [01:27<00:00, 45.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3956 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-6-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1849/1849 [00:40<00:00, 45.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849 items saved to E:\\tests\\1sec-auto\\recipe-1\\spectro-6-db.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For each spectro file:\n",
    "for idex, spec in enumerate(spectro_files):\n",
    "\n",
    "    spec_file = spectro_folder + '\\\\' + 'spec_config_' + str(idex+1) + '.json'\n",
    "    \n",
    "    # join into a database\n",
    "    # Load the spectrogram representation & parameters, this returns a dict \n",
    "    spec_cfg = load_audio_representation(spec_file, name=\"spectrogram\")\n",
    "\n",
    "    # Create a table called \"train\" in the database, defined by db_name, using the \"train\" selections table, the spectrogram config, and the audio data \n",
    "    # Behind the hood, this creates an AudioLoader and AudioWriter Ketos function which generates the spectrograms for each selection \n",
    "    # For the specific spectrogram in this case, the spectrograms are of size [1500,56] where 56 refers to the frequency dimension and 1500 refers to the time dimension \n",
    "    # The size of the spectrogram is 1500*56, which is 84000\n",
    "    dbi.create_database(output_file=db_names[idex],  # empty brackets\n",
    "                        dataset_name=r'train', selections=train, data_dir=data_folder,\n",
    "                        audio_repres=spec_cfg)\n",
    "    \n",
    "    dbi.create_database(output_file=db_names[idex],  # empty brackets\n",
    "                        dataset_name=r'val', selections=val, data_dir=data_folder,\n",
    "                        audio_repres=spec_cfg)\n",
    "    \n",
    "    dbi.create_database(output_file=db_names[idex],  # empty brackets\n",
    "                        dataset_name=r'test', selections=test, data_dir=data_folder,\n",
    "                        audio_repres=spec_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078286a1-9d5b-44cd-9f6e-37a78e1b89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idex, spec in enumerate(spectro_files): \n",
    "\n",
    "    spec_file = spectro_folder + '\\\\' + 'spec_config_' + str(idex+1) + '.json'\n",
    "    \n",
    "    # join into a database\n",
    "    # Load the spectrogram representation & parameters, this returns a dict \n",
    "    spec_cfg = load_audio_representation(spec_file, name=\"spectrogram\")\n",
    "\n",
    "    # Set the random seed for numpy and tensorflow \n",
    "    np.random.seed(1000)\n",
    "    tf.random.set_seed(2000)\n",
    "    \n",
    "    # Set the batch size and number of epochs for training\n",
    "    batch_size = 16\n",
    "    n_epochs = 40\n",
    "    \n",
    "    # Set the log folder and checkpoint folder \n",
    "    log_folder = spectro_test_folder + '\\\\' + 'logs' + str(idex)\n",
    "    checkpoint_folder = spectro_test_folder + '\\\\' + 'checkpoints' + str(idex)\n",
    "    \n",
    "    # Open the database file in read mode\n",
    "    db = dbi.open_file(db_names[idex], 'r')\n",
    "    \n",
    "    # Open the training and validation tables respectively \n",
    "    train_data = dbi.open_table(db, \"/train/data\")\n",
    "    val_data = dbi.open_table(db, \"/val/data\")\n",
    "    \n",
    "    # Create batches of training data of size batch size, using the specified data table \n",
    "    # This returns indices of the data in each batch along with their labels \n",
    "    train_generator = BatchGenerator(batch_size=batch_size, data_table=train_data,\n",
    "                                        output_transform_func=ResNetInterface.transform_batch,\n",
    "                                        shuffle=True, refresh_on_epoch_end=True)\n",
    "    \n",
    "    # Create batches of validation data of size batch size, using the specified data table \n",
    "    # This returns indices of the data in each batch along with their labels \n",
    "    val_generator = BatchGenerator(batch_size=batch_size, data_table=val_data,\n",
    "                                       output_transform_func=ResNetInterface.transform_batch,\n",
    "                                       shuffle=False, refresh_on_epoch_end=False)\n",
    "    \n",
    "    # Build the ResNet model file based off of the recipe file - this creates a \"ResNetInterface\" object \n",
    "    resnet = ResNetInterface.build(recipe)\n",
    "    \n",
    "    # Set the training and validation generators to the batch generators created above \n",
    "    resnet.train_generator = train_generator\n",
    "    resnet.val_generator = val_generator\n",
    "    \n",
    "    # Set the model log and checkpoint directory \n",
    "    resnet.log_dir = log_folder\n",
    "    resnet.checkpoint_dir = checkpoint_folder\n",
    "    \n",
    "    # Train the model, looping through all of the training and validation data \n",
    "    # See code map for more information\n",
    "    resnet.train_loop(n_epochs=n_epochs, verbose=False, log_csv=True, csv_name='log.csv')\n",
    "    \n",
    "    # Close the database \n",
    "    db.close()\n",
    "    \n",
    "    # Save the model file, and keep track of the spectrogram parameters used to generate that model \n",
    "    resnet.save(model_names[idex], audio_repr_file=spec_file)\n",
    "\n",
    "    print('Done spectro ' + str(idex+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c6acc-6c7a-435b-b79d-76589f092e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ketos_env",
   "language": "python",
   "name": "ketos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
