{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b46d13-630c-4cfd-8077-928a2b4bb23f",
   "metadata": {},
   "source": [
    "## Fine Tuning Pearce Point Ensemble\n",
    "\n",
    "After deploying the \"final\" version of the detector on PP, it was found that there were a lot of false positives caused by bowheads and ice. Here I will create a new database containing negative examples which include ice and bowheads, and fine-tune the existing detector to see if it improves results for pearce point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d583ab-08a0-4df3-ae51-dcd4843fb83a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T23:47:25.134951Z",
     "iopub.status.busy": "2024-06-13T23:47:25.134951Z",
     "iopub.status.idle": "2024-06-13T23:47:36.942039Z",
     "shell.execute_reply": "2024-06-13T23:47:36.942039Z",
     "shell.execute_reply.started": "2024-06-13T23:47:25.134951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done importing packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kzammit\\Miniconda3\\envs\\ketos_env\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix as confusion_matrix_sklearn\n",
    "import scipy\n",
    "\n",
    "from ketos.data_handling import selection_table as sl\n",
    "import ketos.data_handling.database_interface as dbi\n",
    "from ketos.data_handling.parsing import load_audio_representation\n",
    "from ketos.data_handling.data_feeding import BatchGenerator\n",
    "from ketos.neural_networks.resnet import ResNetInterface\n",
    "from ketos.audio.audio_loader import AudioFrameLoader, AudioLoader, SelectionTableIterator\n",
    "from ketos.audio.spectrogram import MagSpectrogram\n",
    "from ketos.neural_networks.dev_utils.detection import batch_load_audio_file_data, filter_by_threshold, filter_by_label, merge_overlapping_detections\n",
    "from ketos.data_handling.data_feeding import JointBatchGen\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print('done importing packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312f3953-7095-43e3-bec1-8de546cdaae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T23:47:36.944031Z",
     "iopub.status.busy": "2024-06-13T23:47:36.943031Z",
     "iopub.status.idle": "2024-06-13T23:47:36.952440Z",
     "shell.execute_reply": "2024-06-13T23:47:36.951638Z",
     "shell.execute_reply.started": "2024-06-13T23:47:36.944031Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_rows(file_durations, table):\n",
    "\n",
    "    drop = []\n",
    "\n",
    "    print('cleaning training table of original length ' + str(len(table)))\n",
    "\n",
    "    for idex, row in table.iterrows():\n",
    "    \n",
    "        # filename is row[0], end time is idex.end\n",
    "        print(row.name[0])\n",
    "        index = file_durations.loc[file_durations['filename'] == row.name[0]].index\n",
    "        duration = file_durations['duration'][index].values[0]\n",
    "    \n",
    "        if duration < row.end:\n",
    "            # drop the row corresponding to that sel_id and filename from the dataframe\n",
    "            drop.append(idex)\n",
    "    \n",
    "        if row.start < 0:\n",
    "            drop.append(idex)\n",
    "    \n",
    "    print('Number of rows to drop (note, one entry may be in list twice): ' + str(len(drop)))\n",
    "\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74eb4d0-5cd5-4414-9baa-83e3ff5bc6e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T23:47:36.952440Z",
     "iopub.status.busy": "2024-06-13T23:47:36.952440Z",
     "iopub.status.idle": "2024-06-13T23:47:37.688112Z",
     "shell.execute_reply": "2024-06-13T23:47:37.687549Z",
     "shell.execute_reply.started": "2024-06-13T23:47:36.952440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           label       start  \\\n",
      "filename                                           sel_id                      \n",
      "E:\\baseline-with-normalization-reduce-tonal\\pea... 0           0  299.878735   \n",
      "                                                   1           0  300.878735   \n",
      "                                                   2           0  301.878735   \n",
      "\n",
      "                                                                  end Class  \n",
      "filename                                           sel_id                    \n",
      "E:\\baseline-with-normalization-reduce-tonal\\pea... 0       300.878735  BO/I  \n",
      "                                                   1       301.878735  BO/I  \n",
      "                                                   2       302.878735  BO/I  \n"
     ]
    }
   ],
   "source": [
    "# Create one second segments from new false annotations \n",
    "annot_neg_pp = pd.read_excel(r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\pearce-point-false-annots.xlsx')\n",
    "std_annot_neg_pp = sl.standardize(table=annot_neg_pp, trim_table=True)\n",
    "pp_neg_add = sl.select(annotations=std_annot_neg_pp, length=1.0, step=1, min_overlap=1, center=False)\n",
    "pp_neg_add['Class'] = 'BO/I'\n",
    "print(pp_neg_add.head(3))\n",
    "pp_neg_add.to_excel(r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\new_negatives_pp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e409da70-b28b-4718-9893-2aa2053cf94c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T23:47:37.689229Z",
     "iopub.status.busy": "2024-06-13T23:47:37.689229Z",
     "iopub.status.idle": "2024-06-13T23:47:40.608720Z",
     "shell.execute_reply": "2024-06-13T23:47:40.608720Z",
     "shell.execute_reply.started": "2024-06-13T23:47:37.689229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filename  sel_id       start  \\\n",
      "0  D:\\ringed-seal-data\\Pearce_Point_2018_2019\\120...       0   26.070746   \n",
      "1  D:\\ringed-seal-data\\Pearce_Point_2018_2019\\120...       1   71.371480   \n",
      "2  D:\\ringed-seal-data\\Pearce_Point_2018_2019\\120...       2  114.012267   \n",
      "\n",
      "          end  label classification Class    dup  \n",
      "0   27.070746      0            NaN    OK  False  \n",
      "1   72.371480      0            NaN     O  False  \n",
      "2  115.012267      0            NaN    OU  False  \n"
     ]
    }
   ],
   "source": [
    "# because of the weird ketos merging, read in the excel sheet instead of using the above\n",
    "annot_neg_pp = r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\new_negatives_pp.xlsx'\n",
    "\n",
    "neg_folder = r'E:\\baseline-with-normalization-reduce-tonal\\annots\\neg'\n",
    "pos_folder = r'E:\\baseline-with-normalization-reduce-tonal\\annots\\pos'\n",
    "db_name = r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\pp-w-false-additions.h5'\n",
    "data_folder = r'D:\\ringed-seal-data'\n",
    "file_durations = pd.read_excel(r'E:\\baseline-with-normalization-reduce-tonal\\all_file_durations_complete.xlsx')\n",
    "\n",
    "# Old negative file\n",
    "pp_old_neg = pd.read_excel(neg_folder + '\\\\' + 'PP-negs-joined.xlsx')\n",
    "pp_old_neg = pp_old_neg.ffill()\n",
    "\n",
    "pp_new_neg = pd.read_excel(annot_neg_pp)\n",
    "pp_new_neg = pp_new_neg.ffill()\n",
    "\n",
    "pp_neg = pd.concat([pp_old_neg, pp_new_neg])\n",
    "\n",
    "print(pp_neg.head(3))\n",
    "pp_neg.to_excel(r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\concat_negs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "203d243b-1735-4046-b3b9-175879aded99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T23:47:40.609714Z",
     "iopub.status.busy": "2024-06-13T23:47:40.609714Z",
     "iopub.status.idle": "2024-06-13T23:47:40.730106Z",
     "shell.execute_reply": "2024-06-13T23:47:40.730106Z",
     "shell.execute_reply.started": "2024-06-13T23:47:40.609714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives standardized? TrueTrue\n"
     ]
    }
   ],
   "source": [
    "pp_tr_neg = pp_neg.head(int(len(pp_neg)*(85/100)))\n",
    "pp_te_neg = pp_neg[~pp_neg.index.isin(pp_tr_neg.index)]\n",
    "\n",
    "pp_tr_neg = sl.standardize(table=pp_tr_neg)\n",
    "pp_te_neg = sl.standardize(table=pp_te_neg)\n",
    "\n",
    "print('Negatives standardized? ' + str(sl.is_standardized(pp_tr_neg)) + str(sl.is_standardized(pp_te_neg)))\n",
    "\n",
    "pp_pos = pd.read_csv(pos_folder + '\\\\' + 'PP_all_formatted_1sec.csv')\n",
    "pp_pos = pp_pos.ffill()\n",
    "pp_tr_pos = pp_pos.head(int(len(pp_pos)*(85/100)))\n",
    "pp_te_pos = pp_pos[~pp_pos.index.isin(pp_tr_pos.index)]\n",
    "\n",
    "pp_tr_pos = sl.standardize(table=pp_tr_pos, start_labels_at_1=True)\n",
    "pp_te_pos = sl.standardize(table=pp_te_pos, start_labels_at_1=True)\n",
    "\n",
    "pp_tr = pd.concat([pp_tr_pos, pp_tr_neg])\n",
    "pp_te = pd.concat([pp_te_pos, pp_te_neg])\n",
    "\n",
    "pp_tr.to_excel(r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\train.xlsx')\n",
    "pp_te.to_excel(r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ec16f6-9925-4117-891e-c97b9d1296b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T23:47:40.731102Z",
     "iopub.status.busy": "2024-06-13T23:47:40.731102Z",
     "iopub.status.idle": "2024-06-13T23:47:57.258702Z",
     "shell.execute_reply": "2024-06-13T23:47:57.258702Z",
     "shell.execute_reply.started": "2024-06-13T23:47:40.731102Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████████████████████████                            | 151/232 [00:14<00:02, 37.51it/s]UserWarning: While processing 1208234033.020419202219.wav: over 50% of the selection falls outside the audio file (299.88s,300.88s).\n",
      "RuntimeWarning: Waveform padded with its own reflection to achieve required length to compute the stft. 0 samples were padded on the left and 2863 samples were padded on the right\n",
      " 67%|█████████████████████████████████████████████████████▊                          | 156/232 [00:14<00:01, 38.70it/s]UserWarning: While processing 1208234033.020419202219.wav: selection start time exceeds file duration (300.88s).\n",
      "RuntimeWarning: Offset exceeds file duration. Empty waveform returned\n",
      "UserWarning: While processing 1208234033.020419202219.wav: selection start time exceeds file duration (301.88s).\n",
      "UserWarning: While processing 1208234033.020818172219.wav: selection start time exceeds file duration (300.23s).\n",
      "UserWarning: While processing 1208234033.020818172219.wav: selection start time exceeds file duration (301.23s).\n",
      " 71%|████████████████████████████████████████████████████████▌                       | 164/232 [00:15<00:01, 47.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tuple' object has no attribute 'stft_args'\n",
      "'tuple' object has no attribute 'stft_args'\n",
      "'tuple' object has no attribute 'stft_args'\n",
      "'tuple' object has no attribute 'stft_args'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 232/232 [00:15<00:00, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 items saved to E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\pp-w-false-additions.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 61.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 items saved to E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\pp-w-false-additions.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spectro_file = r'E:\\baseline-with-normalization-reduce-tonal\\spec_config_100-1200Hz-0.032-hamm-normalized-reduce-tonal.json'\n",
    "spec_cfg = load_audio_representation(spectro_file, name=\"spectrogram\")\n",
    "\n",
    "dbi.create_database(output_file=db_name,  # empty brackets\n",
    "                    dataset_name=r'train', selections=pp_tr, data_dir=data_folder,\n",
    "                    audio_repres=spec_cfg)\n",
    "\n",
    "dbi.create_database(output_file=db_name,  # empty brackets\n",
    "                    dataset_name=r'test', selections=pp_te, data_dir=data_folder,\n",
    "                    audio_repres=spec_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837c8139-d61e-43c4-8fc4-4e1b9792cccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T23:49:02.512062Z",
     "iopub.status.busy": "2024-06-13T23:49:02.512062Z",
     "iopub.status.idle": "2024-06-13T23:49:02.526054Z",
     "shell.execute_reply": "2024-06-13T23:49:02.526054Z",
     "shell.execute_reply.started": "2024-06-13T23:49:02.512062Z"
    }
   },
   "outputs": [],
   "source": [
    "main_folder = r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune'\n",
    "\n",
    "model_folder = r'E:\\baseline-with-normalization-reduce-tonal\\models'\n",
    "\n",
    "pretrained_models = [model_folder + '\\\\' + 'rs-model-0.kt', model_folder + '\\\\' + 'rs-model-1.kt', model_folder + '\\\\' + 'rs-model-2.kt', \n",
    "                     model_folder + '\\\\' + 'rs-model-3.kt', model_folder + '\\\\' + 'rs-model-4.kt', model_folder + '\\\\' + 'rs-model-5.kt',\n",
    "                    model_folder + '\\\\' + 'rs-model-6.kt', model_folder + '\\\\' + 'rs-model-7.kt', model_folder + '\\\\' + 'rs-model-8.kt',\n",
    "                    model_folder + '\\\\' + 'rs-model-9.kt']\n",
    "\n",
    "new_models = [main_folder + '\\\\' + 'rs-model-0-ft.kt', main_folder + '\\\\' + 'rs-model-1-ft.kt', main_folder + '\\\\' + 'rs-model-2-ft.kt', \n",
    "              main_folder + '\\\\' + 'rs-model-3-ft.kt', main_folder + '\\\\' + 'rs-model-4-ft.kt', main_folder + '\\\\' + 'rs-model-5-ft.kt',\n",
    "             main_folder + '\\\\' + 'rs-model-6-ft.kt', main_folder + '\\\\' + 'rs-model-7-ft.kt', main_folder + '\\\\' + 'rs-model-8-ft.kt',\n",
    "              main_folder + '\\\\' + 'rs-model-9-ft.kt']\n",
    "\n",
    "np_seeds = [1736, 680, 1996, 1522, 867, 543, 249, 707, 584, 1236, 161]\n",
    "tf_seeds = [1660, 977, 1396, 1456, 1539, 673, 1743, 1492, 1776, 1273, 394]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d392ea5-83c0-44a5-a350-dc68efa673af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T23:49:03.296396Z",
     "iopub.status.busy": "2024-06-13T23:49:03.296396Z",
     "iopub.status.idle": "2024-06-14T00:03:58.799463Z",
     "shell.execute_reply": "2024-06-14T00:03:58.799463Z",
     "shell.execute_reply.started": "2024-06-13T23:49:03.296396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Set the batch size and number of epochs for training\n",
    "batch_size = 16\n",
    "n_epochs = 80\n",
    "\n",
    "for idx, model in enumerate(pretrained_models):\n",
    "    \n",
    "    # Set the random seed for numpy and tensorflow\n",
    "    np.random.seed(np_seeds[idx])\n",
    "    tf.random.set_seed(tf_seeds[idx])\n",
    "\n",
    "    # Set the log folder and checkpoint folder\n",
    "    log_folder = main_folder + '\\\\' + 'logs' + str(idx)\n",
    "    checkpoint_folder = main_folder + '\\\\' + 'checkpoints' +str(idx)\n",
    "    \n",
    "    # Open the database file in read mode\n",
    "    db = dbi.open_file(db_name, 'r')\n",
    "    \n",
    "    # Open the training and validation tables respectively\n",
    "    train_data = dbi.open_table(db, \"/train/data\")\n",
    "    val_data = dbi.open_table(db, \"/test/data\")\n",
    "    \n",
    "    # Create batches of training data of size batch size, using the specified data table\n",
    "    # This returns indices of the data in each batch along with their labels\n",
    "    train_generator = BatchGenerator(batch_size=batch_size, data_table=train_data,\n",
    "                                        output_transform_func=ResNetInterface.transform_batch,\n",
    "                                        shuffle=True, refresh_on_epoch_end=True)\n",
    "    \n",
    "    # Create batches of validation data of size batch size, using the specified data table \n",
    "    # This returns indices of the data in each batch along with their labels \n",
    "    val_generator = BatchGenerator(batch_size=batch_size, data_table=val_data,\n",
    "                                       output_transform_func=ResNetInterface.transform_batch,\n",
    "                                       shuffle=False, refresh_on_epoch_end=False)\n",
    "    \n",
    "    # Load the pretrained model, replacing the top (aka. classification layers). This method inherently freezes the base.\n",
    "    resnet = ResNetInterface.load(model, replace_top=True)\n",
    "    \n",
    "    # Set the training and validation generators to the batch generators created above\n",
    "    resnet.train_generator = train_generator\n",
    "    resnet.val_generator = val_generator\n",
    "    \n",
    "    # Set the model log and checkpoint directory\n",
    "    resnet.log_dir = log_folder\n",
    "    resnet.checkpoint_dir = checkpoint_folder\n",
    "    \n",
    "    # digging into their scripts for fine tuning\n",
    "    # resnet.py, \"clone with new top\"\n",
    "    \n",
    "    # Train the model, looping through all of the training and validation data\n",
    "    # See code map for more information\n",
    "    resnet.train_loop(n_epochs=n_epochs, verbose=False, log_csv=True, csv_name='log-' + str(idx) +'.csv', validate=True)\n",
    "    #resnet.train_loop(n_epochs=n_epochs, verbose=False, log_csv=True, csv_name='log.csv', validate=False)\n",
    "    \n",
    "    # Close the database\n",
    "    db.close()\n",
    "    \n",
    "    # Save the model file, and keep track of the spectrogram parameters used to generate that model\n",
    "    resnet.save(new_models[idx], audio_repr_file=spectro_file)\n",
    "    \n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2750600-b867-496d-91f8-6361e4687f0b",
   "metadata": {},
   "source": [
    "## Deploy on Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c9c333-1ea1-4d20-972b-5eee88039567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T00:41:03.749747Z",
     "iopub.status.busy": "2024-06-14T00:41:03.749382Z",
     "iopub.status.idle": "2024-06-14T00:41:03.761707Z",
     "shell.execute_reply": "2024-06-14T00:41:03.761707Z",
     "shell.execute_reply.started": "2024-06-14T00:41:03.749747Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch_generator(spectro_file, audio_folder, step_size, batch_size): \n",
    "    \n",
    "    audio_repr = load_audio_representation(path=spectro_file)\n",
    "\n",
    "    spec_config = audio_repr['spectrogram']\n",
    "\n",
    "    audio_loader = AudioFrameLoader(path=audio_folder, duration=spec_config['duration'],\n",
    "                                step=step_size, stop=False, representation=spec_config['type'],\n",
    "                                representation_params=spec_config, pad=False)\n",
    "\n",
    "    batch_generator = batch_load_audio_file_data(loader=audio_loader, batch_size=batch_size)\n",
    "\n",
    "    return batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14847575-76f7-4d8c-8d9d-c05f270d5b70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T00:41:04.003308Z",
     "iopub.status.busy": "2024-06-14T00:41:04.003308Z",
     "iopub.status.idle": "2024-06-14T00:41:04.010582Z",
     "shell.execute_reply": "2024-06-14T00:41:04.010406Z",
     "shell.execute_reply.started": "2024-06-14T00:41:04.003308Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_models(model_names, temp_folders):\n",
    "    \n",
    "    models = []\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        models.append(ResNetInterface.load(model_file=model_name, new_model_folder=temp_folders[idx]))\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e6a6eab-8bb1-4a49-b5e6-3f375fd4ace9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T00:41:04.419223Z",
     "iopub.status.busy": "2024-06-14T00:41:04.419223Z",
     "iopub.status.idle": "2024-06-14T00:41:04.430798Z",
     "shell.execute_reply": "2024-06-14T00:41:04.430798Z",
     "shell.execute_reply.started": "2024-06-14T00:41:04.419223Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_detections(batch_generator, models, output_dir, threshold, raven_txt, audio_folder):\n",
    "    \n",
    "    detections_pos = pd.DataFrame()\n",
    "    detections_neg = pd.DataFrame()\n",
    "\n",
    "    for ibx, batch_data in enumerate(batch_generator):\n",
    "\n",
    "        for idx, model in enumerate(models):\n",
    "\n",
    "            # Run the model on the spectrogram data from the current batch\n",
    "            batch_predictions = model.run_on_batch(batch_data['data'], return_raw_output=True)\n",
    "\n",
    "            if idx == 0:\n",
    "                # Lets store our data in a dictionary\n",
    "    \n",
    "                raw_output_neg = {'filename': batch_data['filename'], 'start': batch_data['start'],\n",
    "                                  'end': batch_data['end'], '0-0': batch_predictions[:, 0]}\n",
    "                \n",
    "                raw_output_pos = {'filename': batch_data['filename'], 'start': batch_data['start'],\n",
    "                                  'end': batch_data['end'], '1-0': batch_predictions[:, 1]}\n",
    "    \n",
    "            else:\n",
    "                raw_output_neg |= {'0-' + str(idx): batch_predictions[:, 0]}\n",
    "                \n",
    "                raw_output_pos |= {'1-' + str(idx): batch_predictions[:, 1]}\n",
    "            \n",
    "        detections_pos = pd.concat([detections_pos, pd.DataFrame.from_dict(raw_output_pos)])\n",
    "        detections_neg = pd.concat([detections_neg, pd.DataFrame.from_dict(raw_output_neg)])\n",
    "\n",
    "    detections_pos.to_excel(output_dir + '\\\\' + 'detections-pos.xlsx', index=False)\n",
    "    detections_neg.to_excel(output_dir + '\\\\' + 'detections-neg.xlsx', index=False)\n",
    "\n",
    "    mean_cols_pos = detections_pos.columns[3:]\n",
    "    mean_cols_neg = detections_neg.columns[3:]\n",
    "    \n",
    "    #detections_pos['mean-pos'] = detections_pos[mean_cols_pos].mean(axis=1)\n",
    "    detections_pos['med-pos'] = detections_pos[mean_cols_pos].quantile(0.5, axis=1)\n",
    "    #detections_neg['mean-neg'] = detections_neg[mean_cols_neg].mean(axis=1)\n",
    "    detections_neg['med-neg'] = detections_neg[mean_cols_neg].quantile(0.5, axis=1)\n",
    "    \n",
    "    #merge_df = detections_pos[['filename', 'start', 'end', 'mean-pos']].copy()\n",
    "    merge_df = detections_pos[['filename', 'start', 'end', 'med-pos']].copy()\n",
    "    #merge_df['mean-neg'] = detections_neg['mean-neg']\n",
    "    merge_df['med-neg'] = detections_neg['med-neg']\n",
    "    \n",
    "    scores = []\n",
    "    for row in merge_df.iterrows():\n",
    "        score = [row[1]['med-neg'], row[1]['med-pos']]\n",
    "        #score = [row[1]['mean-neg'], row[1]['mean-pos']]\n",
    "        scores.extend([score])\n",
    "    \n",
    "    dict = {'filename': merge_df['filename'], 'start': merge_df['start'], 'end': merge_df['end'], 'score': scores}\n",
    "    \n",
    "    filter_detections = filter_by_threshold(dict, threshold=threshold)\n",
    "    detections_filtered = filter_by_label(filter_detections, labels=1).reset_index(drop=True)\n",
    "    print(len(detections_filtered))\n",
    "    detections_grp = merge_overlapping_detections(detections_filtered)\n",
    "    print(len(detections_grp))\n",
    "    detections_grp.to_excel(output_dir + '\\\\' + 'detections-filtered-and-grouped.xlsx', index=False)\n",
    "    \n",
    "    results_table = detections_grp\n",
    "    \n",
    "    cols = ['filename']\n",
    "    results_table.loc[:,cols] = results_table.loc[:,cols].ffill()\n",
    "    results_table['Selection'] = results_table.index +1\n",
    "    results_table['View'] = 'Spectrogram 1'\n",
    "    results_table['Channel'] = 1\n",
    "    results_table['Begin Path'] = audio_folder + '\\\\' + results_table.filename\n",
    "    results_table['File Offset (s)'] = results_table.start\n",
    "    results_table = results_table.rename(columns={\"start\": \"Begin Time (s)\", \"end\": \"End Time (s)\", \"filename\": \"Begin File\"})\n",
    "    results_table['Begin File'] = results_table['Begin File']\n",
    "    results_table['Low Freq (Hz)'] = 100\n",
    "    results_table['High Freq (Hz)'] = 1200\n",
    "    \n",
    "    results_table.to_csv(raven_txt, index=False, sep='\\t')\n",
    "\n",
    "    return detections_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d3ed5a-1d7a-4ceb-8348-f57225747f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T00:41:06.038771Z",
     "iopub.status.busy": "2024-06-14T00:41:06.037812Z",
     "iopub.status.idle": "2024-06-14T00:41:06.044250Z",
     "shell.execute_reply": "2024-06-14T00:41:06.044250Z",
     "shell.execute_reply.started": "2024-06-14T00:41:06.037812Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_one_min_dets(detections_grp, output_dir):\n",
    "\n",
    "    one_min_det = pd.DataFrame(columns=['filename', '0-60s', '60-120s', '120-180s', '180-240s', '240+s'])\n",
    "    all_files = np.unique(detections_grp['filename'])\n",
    "    one_min_det['filename'] = all_files\n",
    "    one_min_det.set_index('filename', inplace=True)\n",
    "    one_min_det = one_min_det.fillna(0)\n",
    "    \n",
    "    for file in detections_grp['filename'].unique():\n",
    "    \n",
    "        temp = detections_grp[detections_grp['filename']==file]\n",
    "        for row in temp.iterrows():\n",
    "            if row[1].end < 60:\n",
    "                one_min_det.at[file, '0-60s'] = one_min_det.loc[file]['0-60s'] + 1\n",
    "            elif row[1].start >= 60 and row[1].end < 120:\n",
    "                one_min_det.at[file, '60-120s'] = one_min_det.loc[file]['60-120s'] + 1\n",
    "            elif row[1].start >= 120 and row[1].end < 180:\n",
    "                one_min_det.at[file, '120-180s'] = one_min_det.loc[file]['120-180s'] + 1\n",
    "            elif row[1].start >= 180 and row[1].end < 240:\n",
    "                one_min_det.at[file, '180-240s'] = one_min_det.loc[file]['180-240s'] + 1\n",
    "            elif row[1].start >= 240:\n",
    "                one_min_det.at[file, '240+s'] = one_min_det.loc[file]['240+s'] + 1\n",
    "    \n",
    "    one_min_det['total'] = one_min_det.sum(axis=1)\n",
    "    one_min_det.to_excel(output_dir + '\\\\' + 'one-min-dets.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f70f36-bf64-4a85-86a8-1b1f5e8ea097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T00:41:10.466608Z",
     "iopub.status.busy": "2024-06-14T00:41:10.466608Z",
     "iopub.status.idle": "2024-06-14T00:41:10.482925Z",
     "shell.execute_reply": "2024-06-14T00:41:10.481358Z",
     "shell.execute_reply.started": "2024-06-14T00:41:10.466608Z"
    }
   },
   "outputs": [],
   "source": [
    "model_folder = r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune\\fine-tuned-models'\n",
    "\n",
    "model_names = [model_folder + \"\\\\\" + \"rs-model-0-ft.kt\", model_folder + \"\\\\\" + \"rs-model-1-ft.kt\", model_folder + \"\\\\\" + \"rs-model-2-ft.kt\",\n",
    "            model_folder + \"\\\\\" + \"rs-model-3-ft.kt\", model_folder + \"\\\\\" + \"rs-model-4-ft.kt\", model_folder + \"\\\\\" + \"rs-model-5-ft.kt\",\n",
    "            model_folder + \"\\\\\" + \"rs-model-6-ft.kt\", model_folder + \"\\\\\" + \"rs-model-7-ft.kt\", model_folder + \"\\\\\" + \"rs-model-8-ft.kt\",\n",
    "            model_folder + \"\\\\\" + \"rs-model-9-ft.kt\"]\n",
    "\n",
    "temp_folders = [model_folder + \"\\\\\" + \"temp-0\", model_folder + \"\\\\\" + \"temp-1\", model_folder + \"\\\\\" + \"temp-2\",\n",
    "            model_folder + \"\\\\\" + \"temp-3\", model_folder + \"\\\\\" + \"temp-4\", model_folder + \"\\\\\" + \"temp-5\",\n",
    "            model_folder + \"\\\\\" + \"temp-6\", model_folder + \"\\\\\" + \"temp-7\", model_folder + \"\\\\\" + \"temp-8\",\n",
    "            model_folder + \"\\\\\" + \"temp-9\"]\n",
    "\n",
    "spectro_file = r'E:\\baseline-with-normalization-reduce-tonal\\spec_config_100-1200Hz-0.032-hamm-normalized-reduce-tonal.json'\n",
    "\n",
    "# Step 0.5s each time (overlap of 50% for 1 sec duration)\n",
    "step_size = 0.5\n",
    "\n",
    "# Number of samples in batch\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dabd103-9322-486f-acb5-c7b2868f9dcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T00:41:10.721609Z",
     "iopub.status.busy": "2024-06-14T00:41:10.721609Z",
     "iopub.status.idle": "2024-06-14T00:41:10.730161Z",
     "shell.execute_reply": "2024-06-14T00:41:10.729608Z",
     "shell.execute_reply.started": "2024-06-14T00:41:10.721609Z"
    }
   },
   "outputs": [],
   "source": [
    "main_folder = r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\fine-tune'\n",
    "\n",
    "audio_folder = r'E:\\baseline-with-normalization-reduce-tonal\\pearce-point\\audio'\n",
    "\n",
    "# Threshold\n",
    "threshold = 0.5\n",
    "\n",
    "output_dir = main_folder\n",
    "detections_csv = output_dir + '\\\\' + 'detections-avg.csv'\n",
    "temp_folder = output_dir + '\\\\' + 'ringedS_tmp_folder'\n",
    "pos_detection = output_dir + '\\\\' + 'grouped-filtered-dets.xlsx'\n",
    "raven_txt = output_dir + '\\\\' + 'raven-formatted-detections.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e95405f5-9d3b-4dec-9c91-599c38ef4571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T00:41:11.685094Z",
     "iopub.status.busy": "2024-06-14T00:41:11.685094Z",
     "iopub.status.idle": "2024-06-14T00:44:10.365634Z",
     "shell.execute_reply": "2024-06-14T00:44:10.365342Z",
     "shell.execute_reply.started": "2024-06-14T00:41:11.685094Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/748 [00:00<?, ?it/s]RuntimeWarning: Waveform padded with its own reflection to achieve required length to compute the stft. 46 samples were padded on the left and 0 samples were padded on the right\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002781D814DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002781D053700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 748/748 [02:54<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "batch_generator = get_batch_generator(spectro_file, audio_folder, step_size, batch_size)\n",
    "all_models = load_models(model_names, temp_folders)\n",
    "detections_grp = get_detections(batch_generator, all_models, output_dir, threshold, raven_txt, audio_folder)\n",
    "get_one_min_dets(detections_grp, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd0d40-a746-4ed4-bb96-91210c626b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ketos_env",
   "language": "python",
   "name": "ketos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
